[{"content":"How do we specify the values to be used in tests? The values assigned to the attributes of objects in our test fixture and the expected outcome of our test are frequently interconnected, as defined in the requirements. It is vital to accurately determine these values and understand the relationship between the pre-conditions and post-conditions. This relationship plays a critical role in ensuring the correct behavior is incorporated into the System Under Test (SUT).\nLiteral We use literal constants for object attributes and assertions Literal Values are a popular way to specify the values of attributes of objects in a test. Using a Literal Value in-line makes it very clear which value is being used. There is no doubt about the value’s identity because it is right in front of our face. Unfortunately, using Literal Values can make it difficult to see the relationships between the values used in various places in the test, which may in turn lead to Obscure Tests. When the same value needs to be used in several places in the test (typically during fixture setup and result verification), this approach can obscure the relationship between the test pre-conditions and post-conditions It certainly makes sense to use Literal Values if the testing requirements specify which values are to be used and we want to make it clear that we are, in fact, using those values.\nOne downside of using a Literal Value is that we might use the same value for two unrelated attributes; if the SUT happens to use the wrong one, tests may pass even though they should not. If the Literal Value is a filename or a key used to access a database, the meaning of the value is lost—the content of the file or record actually drives the behavior of the SUT. Using a Literal Value as the key does nothing to help the reader understand the test in such a case, and we are likely to suffer from Obscure Tests.\nIf the values in the expected outcome can be derived from the values in the fixture setup logic, we will be more likely to use the Tests as Documentation if we use Derived Values. Conversely, if the values are not important to the specification of the logic being tested, we should consider using Generated Values\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Test{ public void aTest (){ Player player = new Player(\u0026#34;Name\u0026#34;, \u0026#34;Surname\u0026#34;); player.jersey = new Jersey(\u0026#34;Initials\u0026#34;, 10); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(\u0026#34;Name\u0026#34;); assertThat(actualPlayer.surname).isEqual(\u0026#34;Surname\u0026#34;); assertThat(actualPlayer.jersey.number).isEqual(10); assertThat(actualPlayer.jersey.initials).isEqual(\u0026#34;Initials\u0026#34;); assertThat(actualPlayer.fullName).isEqual(\u0026#34;Name Surname\u0026#34;); } } Cause: Irrelevant Information The test exposes a lot of irrelevant details about the fixture that distract the test reader from what really affects the behavior of the SUT.\nAs test readers, we find it hard to determine which of the values passed to objects actually affect the expected outcome. A test contains a lot of data, either as Literal Values or as variables. Irrelevant Information often occurs in conjunction with Hard-Coded Test Data or a General Fixture but can also arise because we make visible all data the test needs to execute rather than focusing on the data the test needs to be understood\nCause: Hard-Coded Test Data Data values in the fixture, assertions, or arguments of the SUT are hard-coded in the Test Method, obscuring cause–effect relationships between inputs and expected outputs. As test readers, we find it difficult to determine how various hard-coded (i.e., literal) values in the test are related to one another and which values should affect the behavior of the SUT. We may also encounter behavior smells such as Erratic Tests.\nHard-Coded Test Data occurs when a test contains a lot of seemingly unrelated Literal Values. Tests When we use “cut-and-paste” reuse of test logic, we find ourselves replicating the literal values to the derivative tests.\nSymbolic Constant To reduce duplication we can Symbolic Constant (local constant):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test { public void aTest() { String name = \u0026#34;Name\u0026#34;; String surname = \u0026#34;Surname\u0026#34;; Player player = new Player(name, surname); String initials = \u0026#34;Initials\u0026#34;; int jerseyNumber = 10; player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(name); assertThat(actualPlayer.surname).isEqual(surname); assertThat(actualPlayer.jersey.number).isEqual(jerseyNumber); assertThat(actualPlayer.jersey.initials).isEqual(initials); String nameSurname = \u0026#34;Name Surname\u0026#34;; assertThat(actualPlayer.fullName).isEqual(nameSurname); } } Self-Describing Value To make it more readable useful to choose a value that describes the role of the value in the test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test{ public void aTest (){ String name = \u0026#34;Andres\u0026#34;; String surname = \u0026#34;Iniesta\u0026#34;; Player player = new Player(name, surname); String initials = \u0026#34;AS\u0026#34;; int jerseyNumber = 10; player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(name); assertThat(actualPlayer.surname).isEqual(surname); assertThat(actualPlayer.jersey.number).isEqual(10); assertThat(actualPlayer.jersey.initials).isEqual(initials); String nameSurname = \u0026#34;Andres Iniesta\u0026#34;; assertThat(actualPlayer.fullName).isEqual(nameSurname); } } Derived Value We use expressions to calculate values that can be derived from other values Often, some of these values can be derived from other values in the same test. In these cases the benefits from using our Tests as Documentation are improved if we show the derivation by calculating the values using the appropriate expression.\nComputers excel at math and string concatenation, allowing us to code expected results directly into tests using Assertion Method calls. Derived Values are useful for creating fixture objects and exercising the SUT, encouraging the use of variables or constants to hold values. These variables/constants can be initialized at compile time, during initialization, fixture setup, or within the Test Method itself.\nWe should use a Derived Value whenever we have values that can be derived in some deterministic way from other values in our tests. The main drawback of using Derived Values is that the same math error (e.g., rounding errors) could appear in both the SUT and the tests. To be safe, we might want to code a few of the pathological test cases using Literal Values just in case such a problem might be present. If the values we are using must be unique or don’t affect the logic in the SUT, we may be better off using Generated Values instead. We can use a Derived Value either as part of fixture setup (Derived Input or One Bad Attribute) or when determining the expected values to be compared with those generated by the SUT (Derived Expectation).\nWhen the SUT\u0026rsquo;s output should be related to the input values, we can dynamically derive the expected value during the test execution. This eliminates the need for precalculated Literal Values and allows us to use an Equality Assertion to verify the result.\nDerived Input Sometimes our test fixture contains similar values that the SUT might compare or use to base its logic on the difference between them. This operation makes the relationship between the two values explicit. See initials\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test{ public void aTest (){ String name = \u0026#34;Andres\u0026#34;; String surname = \u0026#34;Iniesta\u0026#34;; Player player = new Player(name, surname); String initials = String.valueOf(name.charAt(0)) + surname.charAt(0); int jerseyNumber = 10; player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(name); assertThat(actualPlayer.surname).isEqual(surname); assertThat(actualPlayer.jersey.number).isEqual(10); assertThat(actualPlayer.jersey.initials).isEqual(initials); String nameSurname = \u0026#34;Andres Iniesta\u0026#34;; assertThat(actualPlayer.fullName).isEqual(nameSurname); } } Derived Expectation When some value produced by the SUT should be related to one or more of the values we passed in to the SUT as arguments or as values in the fixture, we can often derive the expected value from the input values as the test executes rather than using precalculated Literal Values. We then use the result as the expected value in an Equality Assertion. To make this test more readable, we can replace any Literal Values that are actually derived from other values with formulas that calculate these values. See assertions derived from input player. This way the local variable are on the top, use only at the creation of the fixture where the expected values are derived\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Test{ public void aTest (){ String name = \u0026#34;Andres\u0026#34;; String surname = \u0026#34;Iniesta\u0026#34;; Player player = new Player(name, surname); String initials = String.valueOf(name.charAt(0)) + surname.charAt(0); int jerseyNumber = 10; player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(player.name); assertThat(actualPlayer.surname).isEqual(player.surname); assertThat(actualPlayer.jersey.number).isEqual(player.jersey.number); assertThat(actualPlayer.jersey.initials).isEqual(player.jersay.initials); assertThat(actualPlayer.fullName).isEqual(String.format(\u0026#34;%s %s\u0026#34;, name, surname)); } } Generated Value We generate a suitable value each time the test is run. When initializing the objects in the test fixture, one issue that must be dealt with is the fact that most objects have various attributes (fields) that need to be supplied as arguments to the constructor. Sometimes the exact values to be used affect the outcome of the test. More often than not, however, it is important only that each object use a different value. When the precise values of these attributes are not important to the test, it is important not to have them visible within the test! Generated Values are used in conjunction with Creation Methods to help us remove this potentially distracting information from the test.\nInstead of deciding which values to use in our tests while we are coding the tests, we generate the values when we actually execute the tests. We can then pick values to satisfy specific criteria such as “must be unique in the database” that can be determined only as the test run unfolds.\nWe use a Generated Value whenever we cannot or do not want to specify the test values until the test is executing. Perhaps the value of an attribute is not expected to affect the outcome of the test and we don’t want to be bothered to define Literal Value\nIn some cases, the SUT requires the value of an attribute to be unique; using a Generated Value can ensure that this criterion is satisfied and thereby prevent Unrepeatable Tests and Test Run Wars by reducing the likelihood of a test conflicting with its parallel incarnation in another test run. Optionally, we can use this distinct value for all attributes of the object; object recognition then becomes very easy when we inspect the object in a debugger. One thing to be wary of is that different values could expose different bugs.\nOne thing to be wary of is that different values could expose different bugs. For example, a single-digit number may be formatted correctly, whereas a multidigit number might not (or vice versa). Generated Values can result in Nondeterministic Tests; if we encounter nondeterminism (sometimes the test passes and then fails during the very next run), we must check the SUT code to see whether differences in value could be the root cause.\nIn general, we shouldn’t use a Generated Value unless the value must be unique because of the nondeterminism such a value may introduce. The obvious alternative is to use a Literal Value. A less obvious alternative is to use a Derived Value, especially when we must determine the expected results of a test.\nRandom Generated Values One way to obtain good test coverage without spending a lot of time analyzing the behavior and generating test conditions is to use different values each time we run the tests. Using a Random Generated Value is one way to accomplish this goal. While use of such values may seem like a good idea, it makes the tests nondeterministic (Nondeterministic Tests) and can make debugging failed tests very difficult. Ideally, when a test fails, we want to be able to repeat that test failure on demand. To do so, we can log the Random Generated Value as the test is run and show it as part of the test failure. We then need to find a way to force the test to use that value again while we are troubleshooting the failed test. In most cases, the effort required outweighs the potential benefit. Of course, when we need this technique, we really need it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test{ public void aTest (){ String name = StringUtil.randomString(); String surname = StringUtil.randomString(); int jerseyNumber = IntegerUtil.randomNumber(); Player player = new Player(name, surname); String initials = String.valueOf(name.charAt(0)) + surname.charAt(0); player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(player.name); assertThat(actualPlayer.surname).isEqual(player.surname); assertThat(actualPlayer.jersey.number).isEqual(player.jersey.number); assertThat(actualPlayer.jersey.initials).isEqual(player.jersay.initials); assertThat(actualPlayer.fullName).isEqual(String.format(\u0026#34;%s %s\u0026#34;, name, surname)); } } Distinct Generated Values When we need to ensure that each test or object uses a different value, we can take advantage of Distinct Generated Values. The values generated in each test are the same for each run and can simplify debugging. This adds value and meaning to random generated values. It\u0026rsquo;s easier to debug. See Faker\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test{ static int counter = 0; public void aTest (){ String name = Faker.anyName(counter); //Would return for example: Robert1 or Lionel2 String surname = Faker.anySurname(); int jerseyNumber = Faker.anyNumberBetween(1,99); Player player = new Player(name, surname); String initials = String.valueOf(name.charAt(0)) + surname.charAt(0); player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(player.name); assertThat(actualPlayer.surname).isEqual(player.surname); assertThat(actualPlayer.jersey.number).isEqual(player.jersey.number); assertThat(actualPlayer.jersey.initials).isEqual(player.jersay.initials); assertThat(actualPlayer.fullName).isEqual(String.format(\u0026#34;%s %s\u0026#34;, name, surname)); } } Related Generated Value With a Related Generated Value, all fields of the object contain “related” data, which makes the object easier to recognize when debugging. Another option is to separate the generation of the root from the generation of the values. Another nice touch for strings is to pass a role-describing argument to the function that is combined with the unique integer key to make the code more intent-revealing. Although we could also pass such arguments to the other functions, of course we wouldn’t be able to build them into an integer value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test{ static int counter = 0; public void aTest (){ String name = Faker.uniqueString(\u0026#34;Andres\u0026#34;); //Would return for example: Andres1 or Andres2 String surname = Faker.uniqueString(\u0026#34;Iniesta\u0026#34;); //Generated values are related to a base input int jerseyNumber = Faker.anyNumberBetween(1,99); Player player = new Player(name, surname); String initials = String.valueOf(name.charAt(0)) + surname.charAt(0); player.jersey = new Jersey(initials, jerseyNumber); when(repo.findBy(player.jersay.number)).thenReturn(player); //test eq() vs any() PlayerDto actualPlayer = sut.getPlayer(); assertThat(actualPlayer.name).isEqual(player.name); assertThat(actualPlayer.surname).isEqual(player.surname); assertThat(actualPlayer.jersey.number).isEqual(player.jersey.number); assertThat(actualPlayer.jersey.initials).isEqual(player.jersay.initials); assertThat(actualPlayer.fullName).isEqual(String.format(\u0026#34;%s %s\u0026#34;, name, surname)); } } ","permalink":"https://dcebotarenco.github.io/posts/value-patterns/","summary":"How do we specify the values to be used in tests? The values assigned to the attributes of objects in our test fixture and the expected outcome of our test are frequently interconnected, as defined in the requirements. It is vital to accurately determine these values and understand the relationship between the pre-conditions and post-conditions. This relationship plays a critical role in ensuring the correct behavior is incorporated into the System Under Test (SUT).","title":"Testing Value Patterns"},{"content":"My opinion based on references:\nxUnit Test Patterns book by Gerard Meszaros Test Drive Development: By Example by Kent Beck. Growing Object-Oriented Software, Guided by Tests by Steve Freeman, Nat Pryce Object Mother ObjectMother - Easing Test Object Creation in XP The post contains text snippets from books.\nTestcase Class per what? - Fixture Strategy Management How do we organize our Test Methods onto Testcase Classes? There are a couple of ways to organize tests. This will change over the life of our project. A method is part of a Testcase Class. Should we put all our Test Methods onto a single Testcase Class for the application? Or should we create a Testcase Class for each Test Method? Of course, the right answer lies somewhere between these two extremes, and it will change over the life of our project.\nTestcase Class per Class Testcase Class per Feature Testcase Class per Fixture Testcase Super class Test Helper Testcase Class per Class We put all the Test Methods for one SUT class onto a single Testcase Class When we write our first few Test Methods, we can put them all onto a single Testcase Class. As the number of Test Methods increases, we will likely want to split the Testcase Class so that one Testcase Class per Class is tested, which reduces the number of Test Methods per class . As those Testcase Classes get too big, we usually split the classes further(do we?). In that case, we need to decide which Test Methods to include in each Testcase Class.\nTestcase Class per Feature We group the Test Methods onto Testcase Classes based on which testable feature of the SUT they exercise. As the number of Test Methods increases, we must decide which Testcase Class to assign each Test Method, impacting our ability to grasp the overall test structure. It also influences our fixture setup approach. Using a Testcase Class per Feature allows us to divide a large Testcase Class into smaller ones systematically, without modifying the Test Methods.\nThis approach is suitable when we have a substantial number of Test Methods and want to emphasize the specification of each SUT feature. However, it doesn\u0026rsquo;t simplify or enhance the understanding of individual Test Methods; only Testcase Class per Fixture accomplishes that. Additionally, if each SUT feature only requires one or two tests, it\u0026rsquo;s more practical to stick with a single Testcase Class per Class. Note that having a large number of features on a class is a “smell” indicating the possibility that the class might have too many responsibilities.\nTestcase Class per Fixture We organize Test Methods into Testcase Classes based on commonality of the test fixture. An alternative perspective suggests grouping Test Methods that share the same test fixture into one Testcase Class per Fixture. This allows for centralized fixture setup code in the setUp method, but may scatter test conditions across multiple Testcase Classes.\nOrganizing Test Methods based on the required test fixture simplifies individual tests by utilizing Implicit Setup. The Testcase Class per Fixture pattern is suitable when multiple Test Methods need an identical fixture and emphasizes simplicity. However, if each test requires a unique fixture, this pattern becomes less practical, resulting in numerous single-test classes. In such cases, Testcase Class per Feature or Testcase Class per Class may be more appropriate.\nAn advantage of Testcase Class per Fixture is its ability to easily identify if all operations from each starting state are being tested. This pattern aids in discovering Missing Unit Tests prior to production, especially when viewing an outline or method browser in an IDE.\nTestcase Class per Fixture aligns with behavior-driven development and promotes concise test methods with a focus on a single assertion. Combined with descriptive test method names, this pattern facilitates tests serving as documentation. A side effect is an increased number of Testcase Classes, which can be grouped using nested folders, packages, or namespaces dedicated to these test classes.\nTestcase Super class We group the Test Methods onto Testcase Classes based on which testable feature of the SUT they exercise. When encountering the need for repeated logic in multiple tests, we can address it by utilizing Test Utility Methods. The question then arises: Where should these Test Utility Methods be placed?\nOne option is to use a Testcase Superclass as a centralized location for Test Utility Methods. By subclassing this Testcase Superclass, we can reuse the utility methods across multiple Testcase Classes. This approach assumes that our programming language supports inheritance, there are no conflicting uses of inheritance, and the Test Utility Method does not require access to specific types that are not visible from the Testcase Superclass. The same tells us Kent Beck in his chapter \u0026ldquo;Fixture\u0026rdquo;.\nEach new kind of fixture should be a new subclass of TestCase.\nTest Helper We define a helper class to hold any Test Utility Methods we want to reuse in several tests. We can use a Test Helper if we wish to share logic or variables between several Testcase Classes and cannot (or choose not to) find or define a Testcase Superclass from which we might otherwise subclass all tests that require this logic.\nThe decision between Testcase Superclass and Test Helper all comes down to type visibility. The client classes need to be able to see the Test Utility Method and the Test Utility Method needs to be able to see all the types and classes it depends on. When it doesn\u0026rsquo;t depend on many types or when everything it depends on is visible from a single place, like a DTO in a package on the boundary of the application, the Test Utility Method can be put into a common Testcase Superclass we define for that controller. If it depends on types/classes that cannot be seen from a single place that all the clients can see, it may be necessary to put it on a Test Helper in the appropriate test package or subsystem.\nVariation: Object Mother The xUnit book mentions multiple variations of the Test Helper. But I\u0026rsquo;d like to pay attention a bit to the variation: Object Mother. In my opinion, this particular topic is tricky. Gerard Meszaros says:\nThe Object Mother pattern is simply an aggregate of several other patterns, each of which makes a small but significant contribution to making the test fixture easier to manage. The Object Mother consists of one or more Test Helpers that provide Creation Methods and Attachment Methods , which our tests then use to create ready-to-use test fixture objects. Object Mothers often provide several Creation Methods that create instances of the same class, where each method results in a test object in a different starting state. Because there is no single, crisp definition of what someone means by “Object Mother,” it is advisable to refer to the individual patterns when referring to specific capabilities of the Object Mother.\nMartin says (https://martinfowler.com/bliki/ObjectMother.html):\nThe first move is to create fixture in the setup method of an xunit test - that way it can be reused in multiple tests. But the trouble with this is often you need similar data in multiple test classes. At this point it makes sense to have a factory object that can return standard fixtures.\nHere \u0026ldquo;stardard fixture\u0026rdquo; contradicts the \u0026ldquo;reuse the design of the text fixture across the many tests\u0026rdquo; from xUnit. But then Martin acknowledges that Object Mother will couple your code to the standard fixture.\nObject Mothers do have their faults. In particular there\u0026rsquo;s a heavy coupling in that many tests will depend on the exact data in the mothers. As a result it\u0026rsquo;s tricky should you want to change that standard data for any reason.\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.4710\u0026amp;rep=rep1\u0026amp;type=pdf\nThe purpose of the pattern is to generate business objects that resemble as closely as possible actual objects that will exist in production.\nIn my opinion, Object Mother is the last resort of creation an object in a valid state. It should create the object in a minimal valid state for the test. I see some disadvantages in Object Mother pattern:\nEach time developers want a new different variation of data, a new factory method is created, making the Object Mother big and hard to learn. Sometimes developers use the creation methods to create on object for all use cases like ObjectMother.complete() or ObjectMother.someObject(). This return the object with all the fields populated, but it is unclear in what state objects are created without diving in the external class. Thus these methods end up being used across the code base. In an active development, the model changes over a day as well as Mother Objects. Thus, we would have to maintain additional classes and the correctness of the business objects created. As the original paper says, we would have potentially to sign off the real objects with the analysts, if we have this luxury. Because of this, on one of our projects, we actually had developers pairing with analysts in order to ensure that the objects being generated by the pattern were as close as possible to the real thing.\nLooking the ObjectMother - Easing Test Object Creation in XP authors work with plain POJOs, default constructors, setters and getter. While the process of creating an object can indeed be extracted in factory methods, attachment methods, in my opinion, and some pieces of code could be part of the constructor with required arguments, as well as attachment methods may be replaced with rich domain model vs anemic model. An alternative to Object Mother maybe be the Test Data Builders idea from Growing Object-Oriented Software, Guided by Test Choosing a Test Method Organization Strategy Clearly, there is no single best practice we can always follow. The best practice is the one that is most appropriate for the particular circumstance.\nTestcase Class by Class is very popular and the default way of grouping tests. When we have a lot of Mockito stubs in each test method, and it grows, and we tend to copy/paste the stubs in each test, any you cannot push them on the class level because you will start guessing which is used when, its clear that something is not good, we have multiple fixtures in one class, we have to take action.\nTestcase Class per Fixture is commonly used when we are writing unit tests for stateful objects and each method needs to be tested in each state of the object.\nTestcase Class per Feature is more appropriate when we are writing customer tests against a Service Facade; It enables us to keep all the tests for a customer-recognizable feature together. When each test needs a slightly different fixture, the right answer may be to select the Testcase Class per Feature pattern.\nConclusion Having the Testcase per Fixture, I will share common utility methods in an Testcase Super class shared across the classes of that use case. If the use case has negative flows, I would move some common methods in the super class apply.\nI can share the ObjectMother but not as a global static method. It should be either package private limited to the suite of tests. ObjectMother should have proper method names as in the paper. You should be able to combine different objects with different attachment methods.\nFor object creation, I consider the constructors with required arguments. If the object state grows and differs, and the telescopic constructors emerge, then I will use builder pattern with internal validations in build().\nI would think twice if I have to extract it in a Test Helper aka Object Mother. I rekon there is code duplication, \u0026ldquo;But there are different kinds of duplication\u0026rdquo;. There is an idea from \u0026ldquo;Clean Architecture\u0026rdquo; from Uncle Bob I kinda like:\nThen there is false or accidental duplication. If two apparently duplicated sections of code evolve along different paths—if they change at different rates, and for different reasons—then they are not true duplicates. Now imagine two use cases that have very similar screen structures. The architects will likely be strongly tempted to share the code for that structure. But should they? Is that true duplication? Or it is accidental?\nWe are at the cross roads if we have to share the same data structure and state or not. We either might fall into a standard/general fixture setup for 2 use cases, or we have code duplication in both use cases. Choose wisely..\n","permalink":"https://dcebotarenco.github.io/posts/fixture-strategy-management/","summary":"My opinion based on references:\nxUnit Test Patterns book by Gerard Meszaros Test Drive Development: By Example by Kent Beck. Growing Object-Oriented Software, Guided by Tests by Steve Freeman, Nat Pryce Object Mother ObjectMother - Easing Test Object Creation in XP The post contains text snippets from books.\nTestcase Class per what? - Fixture Strategy Management How do we organize our Test Methods onto Testcase Classes? There are a couple of ways to organize tests.","title":"Fixture Strategy Management"},{"content":"My opinion based on references:\nxUnit Test Patterns book by Gerard Meszaros Test Drive Development: By Example by Kent Beck. Growing Object-Oriented Software, Guided by Tests by Steve Freeman, Nat Pryce Object Mother ObjectMother - Easing Test Object Creation in XP The post contains text snippets from books.\nLet\u0026rsquo;s define \u0026lsquo;fixture\u0026rsquo; The test fixture is everything we need to have to set up in order to exercise the SUT. It includes at least an instance of the class whose method we are testing. We call everything we need in place to exercise the SUT the test fixture, and we call the part of the test logic that we execute to set it up the fixture setup phase of the test. The “test fixture”—or just “fixture”— means “the pre-conditions of the test”.\nFixture types Let me briefly introduce the other types of fixtures. From the persistence perspective:\nFresh Fixture - Each test constructs its own brand-new test fixture for its own private use. Fresh Persistent Fixture - Each test persist the fixture but at the end tears it down Shared Fixture - We reuse the same instance of the test fixture across many tests. It is a persisted fixture From the design perspective:\nMinimal Fixture - We use the smallest and simplest fixture possible for each test Standard Fixture - We reuse the design of the text fixture across the many tests. The Test Smell In my opinion, General fixture test code smell can make developers life complicated and has a big impact on the maintenance of the code in the long run. The project will end-up in situation when the tests are wrote with a lot of straggle or even wrote for coverage and sonar. Let me try to explain why\u0026hellip;\nIt is also known as Standard Fixture. It is related to many other smells and causes like:\nObscure Tests - It is difficult to understand the test at a glance Irrelevant Information - Often occurs in conjunction with Hard-Coded Test Data or a General Fixture but can also arise because we make visible all data the test needs to execute rather than focusing on the data the test needs to be understood. Mystery Guest - The test reader is not able to see the cause and effect between fixture and verification logic because part of it is done outside the Test Method. When either the fixture setup or the result verification part of a test depends on information that is not visible within the test and the test reader finds it difficult to understand the behavior that is being verified without first finding and inspecting the external information, we have a Mystery Guest on our hands. Fragile Fixture - When a Standard Fixture is modified to accommodate a new test, several other tests fail. Fragile Test - A test fails to compile or run when the SUT is changed in ways that do not affect the part the test is exercising. Data Sensitivity - If the data changes, the tests may fail unless great effort has been expended to make them insensitive to the data being used. Context Sensitivity - The behavior of the system may be affected by the state of things outside (e.g system clock) Slow Tests - Tests are consistently slow because each test builds the same over-engineered fixture Many more.. I came across General fixture in combination with the Object Mother at a client. Here is an obfuscated example of code, the complete method is real.\nPseudocode\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package project.space.earth.feature1; public class TestClass1 { @Test void test1() { Foo expected = FooMother.complete(); //general/standard fixture Foo inputFoo = completeFooWithoutIds(); //local fixture out of general/standard fixture given(service.createFoo(inputFoo)).willReturn(expected); //stub Foo actual = sut.insert(inputFoo); //exercise assertThat(actual).usingRecursiveComparison(recursiveComparisonConfiguration).isEqualTo(expected); //verify } private static Foo completeFooWithoutIds() { Bar bar = BarMother.complete().toBuilder().id(null).childId(null).build(); Tee tee = TeeMother.completeBuilder().id(null).field1(null).bars(List.of(bar)).build(); return FooMother.completeBuilder().id(null).field1(null).createdBy(null).createdDate(null).lastModifiedBy(null).lastModifiedDate(null).tee(tee).build(); } } package project.space.in.another.part.of.solar.system.feature2; public class TestClass2 { @Test void test2() { Object object = new Object(); Object anObject = new Object(); Foo existingFoo = FooMother.complete(); //general fixture Bad bar = BarMother.complete().toBuilder().objectList(List.of(object)).build(); //local fixture out of general/standard fixture given(dependency.action(any(), any())).willReturn(anObject);//stub final Foo actual = sut.action(existingFoo, bar); //exercise final Foo expected = existingFoo.toBuilder().object(object).build(); //local fixture out of general/standard fixture assertThat(actual).usingRecursiveComparison(recursiveComparisonConfiguration).isEqualTo(expected); //verify } } package project.space.next.to.jupiter.feature3; public class TestClass3 { //the same situation } package project.space.somewhere.next.to.the.moon.feature4; public class TestClass4 { //the same situation } The complete() method creates an object with all the fields populated as a standard fixture. It is used in 90% of the use cases that we have in our application. The next day, a bug is discovered in one of those use cases that is caused by the general fixture. We change the complete(), and we get out tests failing and other non-related 50 tests also failed. The complete().toBuilder() is available to override default values but then we have to learn and understand the object state to exclude in the current test.\nI think you see the Mystery Guest here, it\u0026rsquo;s hard to understand what is being built. There are no clues about the object\u0026rsquo;s properties state and as a reader you have to jump outside the test context and learn the mystery object.\n.id(null).field1(null).createdBy(null).createdDate(null).lastModifiedBy(null).lastModifiedDate(null)\nThis clearly means the Standard Fixture is not what you want. You build way too much for your test, and you are forced to nullify/revert some standard actions on the object.\nLet\u0026rsquo;s stop here, even tho we can discuss a lot on the code above.\nRoot causes \u0026amp; theory Fixture Strategy Management It all comes from the test fixture management strategy that has a large impact on the execution time and robustness of the tests. The effects of picking the wrong strategy won’t be felt immediately because it takes at least a few hundred tests before the Slow Tests smell becomes evident and probably several months of development before the High Test Maintenance Cost smell starts to emerge.\nSymptoms The symptom is that each of the failed test builds a larger fixture than it should be. Each failed test builds much more that it would appear to be necessary in that test. It is also hard to understand the relationship between the fixture, the SUT and the expected result. We try to create a standard fixture that solves all the current and future use uses of the application. The more diverse the needs of those tests, the more likely we will end up in a General Fixture. In a sense, Standard Fixture is the result of Big Design Upfront of the test fixture for a whole suite of tests.\nImpact This pattern results in a large fixture that grows over time, and it is difficult to understand. It is difficult to understand how each test uses the fixture. The complexity of the fixture violates the Tests as Documentation goal. It can also cause a Fragile Fixture/Fragile Test as people continue to alter the fixture so that it can handle new tests. It can also enable Slow Tests because a larger fixture takes more time to build, especially if a file system of a database gets in the scene.\nMartin Fowler\u0026rsquo;s vision quoted by Gerard When I was reviewing an early draft of this book with Series Editor Martin Fowler, he asked me, “Do people actually do this?” This question exemplifies the philosophical divide of fixture design. Coming from an agile background, Martin lets each test pull a fixture into existence. If several tests happen to need the same fixture, then it makes sense to factor it out into the setUp method and split the class into one Testcase Class per Fixture*. It doesn’t even occur to Martin to design a Standard Fixture that all tests can use. So who uses them?\nIn the xUnit community, use of a Standard Fixture simply to avoid designing a Minimal Fixture for each test is considered undesirable and has been given the name General Fixture.\nMy conclusion A commonly accepted practice is the use of Implicit Setup in conjunction with Testcase Class per Fixture. This approach is suitable when only a few Test Methods share the same fixture design because they require the same setup. In such cases, utilizing a Minimal Fixture can be advantageous to avoid the unnecessary overhead associated with creating objects that are only needed in other tests.\nA Minimal Fixture focuses on using the smallest and simplest fixture possible for each test. By keeping the fixture small and simple, tests become easier to understand compared to fixtures that include unnecessary or irrelevant information. The concept of a Minimal Fixture plays a crucial role in achieving Test as Documentation. To determine if an object is necessary as part of the fixture, one can try removing it. If the test fails as a result, it indicates that the object was likely necessary in some way.\nI would start with a Minimal Fixture in my tests. At the first iteration, I will set up Testcase per Class. Then, if it grows, multiple Mockito.given().thenReturn() (mock mock mock 🦆) and different stubbing occur in each test, I will refactor test code base into Testcase per Fixture.\nHow i\u0026rsquo;d have it, this is a pseudocode, ignoring assertions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package project.space.earth.feature1; public class TestClass1 { @Test void test1() { Foo stub = Foo.builder().id().build(); //minimal required fields given(service.createFoo(any())).willReturn(stub); //minimal stub Foo inputFoo = newFooComposition(); //minimal fixture Foo actual = sut.insert(inputFoo); //exercise //... } private static Foo newFooComposition() { Bar bar = Bar.builder().build(); Tee tee = Tee.builder().build(); return new Foo.builder().bar(bar).tee(tee).build(); } } package project.space.in.another.part.of.solar.system.feature2; public class TestClass2 { @Test void test2() { given(dependency.action(any(), any())).willReturn(new Object());//minimal stub Object object = new Object(); Bad bar = Bar.toBuilder().attachObject(object).build(); //minimal fixture Foo existingFoo = new Foo(); //minimal fixture final Foo actual = sut.action(existingFoo, bar); //exercise //... } } ","permalink":"https://dcebotarenco.github.io/posts/general-fixture/","summary":"My opinion based on references:\nxUnit Test Patterns book by Gerard Meszaros Test Drive Development: By Example by Kent Beck. Growing Object-Oriented Software, Guided by Tests by Steve Freeman, Nat Pryce Object Mother ObjectMother - Easing Test Object Creation in XP The post contains text snippets from books.\nLet\u0026rsquo;s define \u0026lsquo;fixture\u0026rsquo; The test fixture is everything we need to have to set up in order to exercise the SUT. It includes at least an instance of the class whose method we are testing.","title":"General Fixture"},{"content":"Build a Spring-boot Hazelcast cluster in Kubernetes For local testing purposes, you might want to have a cluster of microservices that use Hazelcast where you can watch the replication, rollout of pods and test some Kubernetes infrastructure-related changes.\nPrerequisites Docker minikube kubectl helm minikube Dashboard or Lens Goal Deploy a spring-boot hazelcast cluster locally using helm and connect to it via hazelcast management center Isolate the POC in a separate environment and play with the same microservices and Hazelcast upgrades Limitations We cannot push to any cloud image registries like Azure Container Registry, say it\u0026rsquo;s forbidden by the security policy.\nLocal Hazelcast cluster setup Create a spring boot microservice let\u0026rsquo;s call it my-service\nCode configuration Hazelcast uses by default cluster name dev. For the POC we will name it hazelcast-cluster. We also provide the cluster DNS name hazelcast-dns.default.svc.cluster.local. You can externalize it in a property as well. I also changed the default port to 5702 for sake of the demo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Bean public Config config() { Config config = new Config(); config.setInstanceName(\u0026#34;my-service\u0026#34;); config.setClusterName(\u0026#34;hazelcast-cluster\u0026#34;); config.getMapConfig(\u0026#34;my-cache\u0026#34;).setInMemoryFormat(InMemoryFormat.OBJECT); JoinConfig join = config.getNetworkConfig() .setPort(5702) .getJoin(); join.getMulticastConfig().setEnabled(false); join.getKubernetesConfig() .setEnabled(true) .setProperty(SERVICE_DNS_PROPERTY, \u0026#34;hazelcast-dns.default.svc.cluster.local\u0026#34;) .setProperty(DNS_TIMEOUT_PROPERTY, \u0026#34;5\u0026#34;); return config; } @Bean(name = \u0026#34;cacheInstance\u0026#34;) public HazelcastInstance hazelcastInstance(Config config) { return Hazelcast.getOrCreateHazelcastInstance(config); } Build the jar This part is easy.\n1 gradle clean bootJar Test your jar 1 2 3 4 java -Dspring.profiles.active=dev \\ -DJASYPT_KEY=jasyptCode \\ -Dspring.config.additional-location=file:application/src/main/resources/application-dev.properties \\ -jar application/build/libs/my-service-0.0.0-local.jar Build the image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 FROM openjdk:11-jre-slim ARG APPLICATION_ROOT=/application ARG LOG_DIR=${APPLICATION_ROOT}/logs RUN apt update \u0026amp;\u0026amp; apt install -y dnsutils RUN apt update \u0026amp;\u0026amp; apt install -y iputils-ping RUN mkdir -p ${LOG_DIR} \u0026amp;\u0026amp; \\ chown 1000:1000 ${LOG_DIR} \u0026amp;\u0026amp; \\ chown 1000:1000 ${APPLICATION_ROOT} COPY *.jar /application/app.jar COPY application.properties /application/application.properties RUN chmod 0755 /application \u0026amp;\u0026amp; \\ chmod 0444 /application/app.jar USER 1000:1000 WORKDIR /application ENTRYPOINT [ \u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/application/app.jar\u0026#34; ] EXPOSE 8081 Move the files to the /lib folder, then run from the project root folder:\n1 2 3 docker build --no-cache --progress=plain \\ -t my-service:1.0 \\ -f platform/docker/Dockerfile application/build/libs Run the container:\n1 2 3 4 docker run -p 8080:8081 \\ -e JASYPT_KEY=\u0026#39;xyzZXy123\u0026#39; \\ my-service:1.0 \\ --spring.config.additional-location=file:/application/application.properties You can also mount a volume to not copy everything inside the container 1 2 3 4 5 docker run -p 8080:8081 \\ -e JASYPT_KEY=\u0026#39;jasyptCode\u0026#39; \\ -v \u0026#34;/$(pwd)/application/src/main/resources/application.properties:/application/application.properties\u0026#34; \\ my-service:1.0 \\ --spring.config.additional-location=file:/application/application.properties Check the image You should see the built image my-service:1.0\n1 docker images Start Minikube I assume you have minikube installed.\n1 2 3 minikube start minikube addons enable ingress #for ingress minikube addons enable registry #for registry Publish docker image Usually, Helm will fetch it from a container registry. If we deploy now the Chart, Helm won\u0026rsquo;t see the docker image. We have 2 options:\nThe easy way. Upload the image manually from the host into minikube The hard way. Create a registry. Details here: Registry Docker Registry Load image in minikube 1 minikube image load my-service:1.0 Keep in mind you have 2 docker engines, the local and the one inside minikube, this is why we need to pull and load(upload) images to minikube\u0026rsquo;s docker. You can avoid this by pointing the docker CLI to the minikube\u0026rsquo;s docker and just build and pull right inside the minikube.\n1 2 eval $(minikube -p minikube docker-env) //point docker to internal minikube docker eval $(minikube docker-env --unset) //point docker back to your local docker Helm Chart Chart.yaml\n1 2 3 4 apiVersion: v1 description: A Helm chart for Kubernetes for service my-service name: my-service version: 0.1.0 Some values.yaml\n1 2 3 4 5 6 7 8 9 environment: \u0026#34;local\u0026#34; replicaCount: \u0026#34;2\u0026#34; image: repository: \u0026#34;my-service\u0026#34; // when on local use only the image name tag: \u0026#34;1.0\u0026#34; pullPolicy: \u0026#34;Never\u0026#34; // do not pull it from any repo, load from minikube internals service: externalPort: \u0026#34;8081\u0026#34; internalPort: \u0026#34;8081\u0026#34; The placeholders are automatically resolved by Helm. deployoment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name }} labels: app: {{ .Chart.Name }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app: {{ .Chart.Name }} template: metadata: labels: app: {{ .Chart.Name }} spec: securityContext: runAsUser: 1000 containers: - name: {{ .Chart.Name }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}\u0026#34; args: [ \u0026#34;--spring.config.additional-location=file:/application/application.properties\u0026#34; ] imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: {{ .Values.service.internalPort }} env: - name: JAVA_TOOL_OPTIONS value: -Xms512m -XX:MaxRAMPercentage=75.0 -XX:MaxMetaspaceSize=300m -XX:+PrintFlagsFinal -XshowSettings:vm - name: ENVIRONMENT value: {{ .Values.environment }} - name: JASYPT_KEY value: anotherKey livenessProbe: httpGet: path: /actuator/health/liveness port: {{ .Values.service.internalPort }} scheme: HTTP initialDelaySeconds: 120 periodSeconds: 20 timeoutSeconds: 6 readinessProbe: httpGet: path: /actuator/health/readiness port: {{ .Values.service.internalPort }} scheme: HTTP initialDelaySeconds: 120 periodSeconds: 30 timeoutSeconds: 6 Open access to service under a certain path using rewrite-target, regex group $2 - ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: rules: - host: my-host - http: paths: - path: /mega(/|$)(.*) pathType: Prefix backend: service: name: my-service port: number: 8081 Deploy Helm Chart in minikube Some Helm commands that you might want to use\n1 2 3 4 helm install hazelcast-poc platform/helm/my-service #execute this one helm upgrade hazelcast-poc platform/helm/my-service helm rollback hazelcast-poc helm uninstall hazelcast-poc Shell into one pod and test the DNS. You should see your 2 pods\n1 2 3 4 5 6 7 8 9 10 I have no name!@my-service-679d747874-vklld:/application$ nslookup hazelcast-dns Server: 10.96.0.10 Address: 10.96.0.10#53 Name: hazelcast-dns.default.svc.cluster.local Address: 172.17.0.7 Name: hazelcast-dns.default.svc.cluster.local Address: 172.17.0.6 I have no name!@my-service-679d747874-vklld:/application$ Deploy hazelcast management center 1 minikube image pull hazelcast/management-center Launch the pod\n1 kubectl create deployment hazelcast-center --image=hazelcast/management-center:latest Forward the port 1 2 kubectl get pods kubectl port-forward hazelcast-center-6f9b687779-krvl7 8080:8080 Open management center in browser localhost:8080. If you don\u0026rsquo;t see both members in the cluster, you can downscale the pods, they will appear. Ingress We can also access our service via ingress on path localhost/mega. Before that enable minikube ingress tunneling:\n1 minikube tunnel 1 2 curl http://localhost/mega/actuator/loggers/root # {\u0026#34;effectiveLevel\u0026#34;:\u0026#34;INFO\u0026#34;} Play with the incremental Hazelcast upgrade by building new images and upgrading the Helm chart. Happy Helming\n","permalink":"https://dcebotarenco.github.io/posts/hazelcast-in-minikube/","summary":"Build a Spring-boot Hazelcast cluster in Kubernetes For local testing purposes, you might want to have a cluster of microservices that use Hazelcast where you can watch the replication, rollout of pods and test some Kubernetes infrastructure-related changes.\nPrerequisites Docker minikube kubectl helm minikube Dashboard or Lens Goal Deploy a spring-boot hazelcast cluster locally using helm and connect to it via hazelcast management center Isolate the POC in a separate environment and play with the same microservices and Hazelcast upgrades Limitations We cannot push to any cloud image registries like Azure Container Registry, say it\u0026rsquo;s forbidden by the security policy.","title":"⛅ Hazelcast in minikube"},{"content":"🧑‍🎓 tltr 1 1234 Print is executed multiple times till the first value that matches both filter conditions is found.\n❓Question 1 2 3 4 5 6 //What is the output? IntStream.range(1, 10) .peek(System.out::print) .filter(i -\u0026gt; i \u0026gt; 2) //A .filter(i -\u0026gt; i \u0026gt; 3) //B .findFirst(); 🕵️‍♂️ Explanation Let\u0026rsquo;s simplify it\n1 IntStream.range(1, 10); This won\u0026rsquo;t do anything because there is no terminal operation which triggers the reading from the source stream.\nBy adding the peek, which is an intermediate operation, the behaviour won\u0026rsquo;t change.\n1 IntStream.range(1, 10).peek(System.out::print); This findFirst (terminal operation) evaluates the pipeline.\n1 IntStream.range(1, 10).peek(System.out::print).findFirst(); During the evaluation, the sinks are wrapped (combined) into one Sink. As a result, we get:\npeek Sink filter A Sink filter B Sink findFirst Sink While the sink is not cancelled (in our case findFirst has no value) and there is an element in the stream, the pipeline will try to advance, fetch the next element and pass it to the wrapped sink as above (top-down)\n✅ Walkthrough So we will have something like this (since the stream is not empty):\nThe stream: 1 2 3 4 5 6 7 8 9\n1 is passed to peek sink peek is executed, 1 is printed 1 passed to filter A sink filter A is executed, false is returned (1\u0026gt;2), the sink is not cancelled, continue the while 2 is fetched and passed to peek sink peek is executed, 2 is printed 2 passed to filter A sink filter A is executed, false is returned (2\u0026gt;2), the sink is not cancelled, continue the while 3 is fetched and passed to peek sink peek is executed, 3 is printed 3 passed to filter A sink filter A is executed, true is returned (3\u0026gt;2) 3 passed tofilter B sink filter B is executed, false is returned (3\u0026gt;3), the sink is not cancelled, continue the while 4 is fetched and passed to peek sink peek is executed, 4 is printed 4 passed to filter A sink filter A is executed, true is returned (4\u0026gt;2) 4 passed to filter B sink filter B is executed, true is returned (4\u0026gt;3) 4 passed to findFirst sink saves the value inside the findFirst which means the sink is cancelled, while interrupted 4 returned Another Example Let\u0026rsquo;s move the peek before the findFirst. Almost the same code, and different output\n1 2 3 4 5 6 //What is the output? IntStream.range(1, 10) .filter(i -\u0026gt; i \u0026gt; 2) //A .filter(i -\u0026gt; i \u0026gt; 3) //B .peek(System.out::print) .findFirst(); Output:\n1 4 The sink:\nfilter A Sink filter B Sink peek Sink findFirst Sink ✅ Walkthrough So we will have something like this: The stream: 1 2 3 4 5 6 7 8 9\n1 is passed to filter A sink filter A is executed, false is returned (1\u0026gt;2), the sink is not cancelled, continue the while 2 is fetched and passed to filter A sink filter A is executed, false is returned (2\u0026gt;2), the sink is not cancelled, continue the while 3 is fetched and passed to filter A sink filter A is executed, true is returned (3\u0026gt;2) 3 passed tofilter B sink filter B is executed, false is returned (3\u0026gt;3), the sink is not cancelled, continue the while 4 is fetched and passed to filter A sink filter A is executed, true is returned (4\u0026gt;2) 4 passed tofilter B sink filter B is executed, true is returned (4\u0026gt;3) 4 is passed to peek sink 4 is printed 4 is passed to findFirst sink saves the value inside the findFirst which means the sink is cancelled, while interrupted 4 returned 📚 Read more Sink definition Stream API documentation\n","permalink":"https://dcebotarenco.github.io/posts/stream-api-question/","summary":"🧑‍🎓 tltr 1 1234 Print is executed multiple times till the first value that matches both filter conditions is found.\n❓Question 1 2 3 4 5 6 //What is the output? IntStream.range(1, 10) .peek(System.out::print) .filter(i -\u0026gt; i \u0026gt; 2) //A .filter(i -\u0026gt; i \u0026gt; 3) //B .findFirst(); 🕵️‍♂️ Explanation Let\u0026rsquo;s simplify it\n1 IntStream.range(1, 10); This won\u0026rsquo;t do anything because there is no terminal operation which triggers the reading from the source stream.","title":"🪤 Peek \u0026 Filter Stream API"},{"content":"🧑‍🎓 tltr Infinite while loop was causing the issue. Write unit tests. Test the exit condition of your while loops.\n🐛 Issue We had some kind of serious performance issue on production. The CPU periodically was pegged at 100% and stayed that way until we restarted the Tomcat. It was degrading the overall site performance for quite a while. We had two instances with peak similarly in CPU. Seems like it must be something inside the Tomcat since that\u0026rsquo;s what is eating the CPU.\nHere is the 6-week view: Here is the 12-week view: The immediate fall of the CPU is where we restart the Tomcat. As you see, after the restart, the CPU jumps back on the ladder 🪜 in a couple of days.\n🕵️‍♂️ Investigation 1. Thread dump \u0026amp; top -H On Linux, the top -h \u0026lt;pid\u0026gt; is useful to determine the overall usage by all processes and break it down to certain threads within a process. Our PID: Run kill 3 \u0026lt;pid\u0026gt; on the main PID that\u0026rsquo;s consuming a lot of CPU. This will give you the thread dump. It does not stop the running process, but it stops the world inside the JVM.\nWhat we see is 596 of CPU%. This is related to the number of cores attached to the machine. One core is 100% Convert the PID from top -H from decimal to hex and match it with the nid from the thread dump.\n1 6458 -\u0026gt; nid=0x1936 Not the ideal way to monitor a JVM instance. You should prefer agents like Sematext, JProfiler, YourKit or JFR. But we have what we have 👉\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026#34;http-nio-8080-exec-127\u0026#34; #328 daemon prio=5 os_prio=0 tid=0x00007f9f20020000 lwp=6454 nid=0x1936 runnable [0x00007f9e4ada4000] java.lang.Thread.State: RUNNABLE at java.util.HashMap.hash(HashMap.java:339) at java.util.HashMap.put(HashMap.java:612) at java.util.HashSet.add(HashSet.java:220) at java_util_Set$add$0.call(Unknown Source) at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) \u0026#34;http-nio-8080-exec-156\u0026#34; #39893 daemon prio=5 os_prio=0 tid=0x00007f9f20024000 lwp=18378 nid=0x47ca runnable [0x00007f9e3199e000] java.lang.Thread.State: RUNNABLE at java.util.HashMap.hash(HashMap.java:339) at java.util.HashMap.put(HashMap.java:612) at java.util.HashSet.add(HashSet.java:220) at java_util_Set$add$0.call(Unknown Source) at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) \u0026#34;http-nio-8080-exec-95\u0026#34; #203 daemon prio=5 os_prio=0 tid=0x00007f9f200a9000 lwp=5605 nid=0x15e5 runnable [0x00007f9e48d82000] java.lang.Thread.State: RUNNABLE at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) at com.site.api.Controller$$FastClassBySpringCGLIB$$8fdc140f.invoke(\u0026lt;generated\u0026gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:752) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) \u0026#34;http-nio-8080-exec-120\u0026#34; #321 daemon prio=5 os_prio=0 tid=0x00007f9f20036800 lwp=6351 nid=0x18cf runnable [0x00007f9e4cec3000] java.lang.Thread.State: RUNNABLE at java_util_Set$add$0.call(Unknown Source) at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) at com.site.api.Controller$$FastClassBySpringCGLIB$$8fdc140f.invoke(\u0026lt;generated\u0026gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:752) 2. Traffic I analyzed the traffic at the point of time where the CPU jumps. I got some httpd logs. httpd is in front of Tomcat for PerimeterX compatibility. Example of logs:\n1 2 87.250.224.200 - - [06/Jun/2022:21:17:08 +0000] \u0026#34;GET /the-faulty-endpoint/id-1 HTTP/1.1\u0026#34; 502 341 \u0026#34;https://www.site.com/pages/page-1\u0026#34; \u0026#34;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.268\u0026#34; 120120565 85.45.207.139 - - [06/Jun/2022:21:18:08 +0000] \u0026#34;GET /the-faulty-endpoint/id-2 HTTP/1.1\u0026#34; 502 341 \u0026#34;https://www.site.com/pages/page-1\u0026#34; \u0026#34;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.268\u0026#34; 120122901 At that point in point in time, I saw just a lot of bots (Apple, Google, Yandex, Bing) indexing the pages\n✅ Put things together What we have now is the requests which tell us the URLs that are called and the values in a URLs. The values in the paths are different id-1, id-2, id-n. We also have the code, which we can analyze locally. Code has no tests 😒\nSimplified pseudo-code:\n1 2 3 4 5 6 7 8 int rowsInDB = findBy(\u0026#39;id-from-url\u0026#39;); int buffer = 10; int minRows = Math.min(rowsInDB, rowsInDB + 10); Set randomIndexes = []; while (randomIndexes.size() \u0026lt; buffer) { randomIndexes.add(random.nextInt(minRows)); } At first, I could not spot the bug because I was on the local environment with no data in my H2 database.\nI assembled a list of all the requests within CPU increase timeframe into a CSV file. Launched locally the JMeter, VisualVM, JDK Mission Control and the app connected to the QA database with data and fired all the requests to my local instance. Finally, I noticed locally a CPU increase, which is not going down.\nThere was an id that returned 5 rows from the database. Then I realized that the problem is related to the random.nextInt(minRows) and the Set collection. Here is why:\n1 2 3 4 5 6 7 8 int rowsInDB = findBy(\u0026#39;faulty-id\u0026#39;); //returns 5 int buffer = 10; int minRows = Math.min(rowsInDB, rowsInDB + 10); //here we get 5 Set randomIndexes = []; while (randomIndexes.size() \u0026lt; buffer) { randomIndexes.add(random.nextInt(minRows)); //seed is 5 } The Set will not increase its size because it accepts unique values that are always between 0-4. Set can contain only 0,1,2,3,4. The size of the set will not increase more than 5 where the buffer is always 10 - infinite loop detected.\nWrote a unit test, confirmed the bug, fixed the test, made a hotfix.\n📚 Userfull links How to get thread id and stack IBM best practices Top -H Fastthread.io\n","permalink":"https://dcebotarenco.github.io/posts/high-cpu-usage-while-loop/","summary":"🧑‍🎓 tltr Infinite while loop was causing the issue. Write unit tests. Test the exit condition of your while loops.\n🐛 Issue We had some kind of serious performance issue on production. The CPU periodically was pegged at 100% and stayed that way until we restarted the Tomcat. It was degrading the overall site performance for quite a while. We had two instances with peak similarly in CPU. Seems like it must be something inside the Tomcat since that\u0026rsquo;s what is eating the CPU.","title":"🔥 High CPU usage on faulty endpoint"},{"content":"🧑‍🎓 tltr Configure evictInBackground, maxIdleTime and maxLifeTime to clear connections from the connection pool or retry the call or use RestTemplate\n🐛 Issue At one of my clients, it was decided no more RestTemplate, all move to 🍃 Spring WebClient. I consider this project very interesting, but it comes with a different mindset which developers do not pay attention to:\nNon-blocking Event driven Connection pool Back to the client. After each deployment, in a matter of days, we were getting - the connection reset by the peer. The server on the other side was resetting the connection. Our connection is not valid anymore. Here is the exception example:\n1 2 3 4 reactor.core.Exceptions$ReactiveException: io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer at reactor.core.Exceptions.propagate(Exceptions.java:392) at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97) at reactor.core.publisher.Mono.block(Mono.java:1706) Logs IPs are cleared.\n1 2 3 4 5 6 7 8 9 10 11 ./catalina.out-20211223.gz:2021-12-22 19:04:34.458 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36] REGISTERED ./catalina.out-20211223.gz:2021-12-22 19:04:34.460 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36] CONNECT: rest.apisandbox.server.com/1.1.1.1:443 ...time... ./catalina.out-20211224.gz:2021-12-23 08:26:16.138 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] READ COMPLETE ./catalina.out-20211224.gz:2021-12-23 08:26:16.138 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] EXCEPTION: io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 WARN --- [or-http-epoll-3] r.netty.http.client.HttpClientConnect : [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] The connection observed an error, the request cannot be retried as the headers/body were sent ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] USER_EVENT: SslCloseCompletionEvent(java.nio.channels.ClosedChannelException) ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] INACTIVE ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] UNREGISTERED 🕵️‍♂️ Investigation We can confirm the connection is broken based on the symbols between the IPs:\n1 2 3 4 //Good connection [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] //Bad connection [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] The legend - means the connection is OK ! means the connection is broken 30857d36 the connection id 92 number of re-usages of the connection Example 1 2 3 2021-12-23 08:26:23.069 DEBUG --- [or-http-epoll-2] reactor.netty.http.client.HttpClient : [338e870a-97, L:/0.0.0.0:55864 ! R:rest.apisandbox.server.com/1.1.1.1:443] USER_EVENT: SslCloseCompletionEvent(java.nio.channels.ClosedChannelException) 2021-12-23 08:26:23.069 DEBUG --- [or-http-epoll-2] reactor.netty.http.client.HttpClient : [338e870a-97, L:/0.0.0.0:55864 ! R:rest.apisandbox.server.com/1.1.1.1:443] INACTIVE 2021-12-23 08:26:23.069 DEBUG --- [or-http-epoll-2] reactor.netty.http.client.HttpClient : [338e870a-97, L:/0.0.0.0:55864 ! R:rest.apisandbox.server.com/1.1.1.1:443] UNREGISTERED The problem The connection was registered/created on: 2021-12-22 19:04:34.458 and it was invalidated on 2021-12-23 08:26:16.139. It means it stayed in the connection pool and was re-used for almost 13 hours. At a certain moment, the server, on the other end, reset the connection. Now we have to throw the connection out from the pool.\nA pick in the documentation, we see some properties:\nProperty Description maxIdleTime The time after which the channel is eligible to be closed when idle (resolution: ms). Default: max idle time is not specified maxLifeTime The total lifetime after which the channel is eligible to be closed (resolution: ms). Default: max lifetime is not specified evictInBackground When this option is enabled, each connection pool regularly checks for connections that are eligible for removal according to eviction criteria like maxIdleTime. By default, this background eviction is disabled ✅ Solution We have to tell the reactor what connections are invalid. We can do that by setting up a ConnectionProvider with some properties. By setting the evictInBackground in combination with maxIdleTime we tell the reactor to clear connections idle for 30 seconds on every 120 seconds. Bellow an example.\nWe can also consider a retry mechanism when the connection is invalid and create a new connection or finally get back to good old RestTemplate, which is blocking and uses one connection at the time\nSimplified for bravery.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Configuration class WebClientConfiguration { private static final Logger log = LoggerFactory.getLogger(WebClientConfiguration.class); @Bean WebClient yourWebClient(Environment config) { ConnectionProvider connectionProvider = ConnectionProvider.builder(\u0026#34;http-connection-pool\u0026#34;) .maxConnections(50) //a number of connections .maxIdleTime(Duration.ofSeconds(30))) //max time to stay in the pool in idle state .maxLifeTime(Duration.ofHours(3))) //max lifetime 3 hours then closed .pendingAcquireTimeout(Duration.ofSeconds(60))) //try to aquire for 60 seconds or else fail .evictInBackground(Duration.ofSeconds(120))) //regularly checks for connections each 120 seconds .metrics(true) //for actuator .build(); HttpClient httpClient = HttpClient.create(connectionProvider) .responseTimeout(Duration.ofSeconds(10))) //close connection, when no data was read within 10 seconds after the socket was initialized. .compress(true) .wiretap(HttpClient.class.getName(), LogLevel.DEBUG, AdvancedByteBufFormat.SIMPLE) //print wiretap logs only when HttpClient logger is on DEBUG .metrics(true, Function.identity()); return WebClient.builder() .clientConnector(new ReactorClientHttpConnector(httpClient)) .baseUrl(\u0026#39;baseUrl\u0026#39;) .build(); } } Optional settings maxLifeTime is just a precaution if there is an exchange of data for 3 hours. pendingAcquireTimeout is how much time to try to acquire a connection - the default is 45 seconds. metrics this is handly for monitoring the throughput and connection pool details. responseTimeout this one is about how much time you have to wait for data after the connection was initialiazed\nThat\u0026rsquo;s pretty much it. You just have to monitor the throughput, see if a tweak of settings is needed. The reactor connection pool is pretty the same as a JDBC connection pool.\nBtw, the same strategy can be applied for the famous prematurely closed exception cases. We could also improve the logging by printing the exception and the connection id on the exception stack trace using ClientRequest.logPrefix().\n📚 Read more The documentation How to Avoid Common Mistakes When Using Reactor Netty\n","permalink":"https://dcebotarenco.github.io/posts/reactor-connection-pool/","summary":"🧑‍🎓 tltr Configure evictInBackground, maxIdleTime and maxLifeTime to clear connections from the connection pool or retry the call or use RestTemplate\n🐛 Issue At one of my clients, it was decided no more RestTemplate, all move to 🍃 Spring WebClient. I consider this project very interesting, but it comes with a different mindset which developers do not pay attention to:\nNon-blocking Event driven Connection pool Back to the client. After each deployment, in a matter of days, we were getting - the connection reset by the peer.","title":"🍃 Spring WebClient \u0026 Connection Pool"},{"content":" All companies have a delivery pipeline of the product. Every company can create its own pipeline and choose any strategy to deliver. In this post I’m going to talk about NSIS and Python. I had a task to think about the product deployment in the company I’m working for. Before I got this task, we used to copy paste files via RDP and VPN to the client server and install them manually. Copying the files was implying many risks like:\n🚫 A file is missing!\n🚫 Where is Next, Next, Next, Accept, Install?\n🚫 Developer does not know the procedure of installing the product!\n🚫 What should developer do with the database dump?\n🚫 What environment variables are needed for the product to run?\nFor the client, we can had different solutions that might look like a pipeline:\nWe could build a full continuous delivery to the client server (from Jenkins to Production) We could host client’s product on our hardware or we could deploy it somewhere in a cloud service, like Azure, AWS\u0026hellip; We could create an .exe file and install it on the client server; Continue copy-pasting the files. In order to ensure a full continuous delivery we need either to host the client’s product ourselves or to run the client products in cloud, but this had its own disadvantages for this particular client:\nClient had his intranet and different security policies around it He considered expensive hardware or expensive traffic Client had only Windows virtual machines Internally, at that time, we had a project, which I investigated to see how it was built, written in NSIS that was preparing the DEV environment. Therefore, I decided to have a look at NSIS and build an EXE that would install our product on the client server. Having an .exe does not require many costs and it is relatively simple to do next, next, next and install for everyone from service department.\nNSIS is a script-driven Installer authoring tool for Microsoft Windows with minimal overhead backed by Nullsoft, the creators of Winamp. NSIS is released under a combination of free software licenses, primarily the zlib license. Well, after a couple of weeks, I’ve written the installer and it looks like this:\nEach customer project has a configuration file, in which the developer fills what third party components the project needs. Everything was fine, we used the NSIS installer to set up a couple of projects, but then we realized that nobody except me knew how this works and how NSIS looks like. I was always asked to have a look at why something was not installed or crashed during the installation (not the perfect code). Why? Because of NSIS. For other developers this was a quite unknown scripting language. In addition, I can say that this is also a complex language with a complex syntax. We were all Java developers that’s why everyone avoided it.\nWhy was NSIS a pain in the 🍑? Pop, Push, Exch… The Stack and shared $0-$9 $R0-$R9 registers There is a stack where you have to push and pop values and a couple of registers where you can save some specific values. I was using the registers for time and date because I’ve created a custom logger in NSIS, which was writing logs into a file at runtime. I did not like NSIS because it does not have variables that we are used to in Java.\nComplex looping in stack When it came up to use some loops, I realized that this is very different from a simple PASCAL loop. Even in PASCAL, it is easier to write a loop. Here is an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 function FileScanner.searchFNC pop $FileScanner.callBackAddress pop $FileScanner.getMask pop $FileScanner.directoryToSearch ${locate::Open} \u0026amp;quot;$FileScanner.directoryToSearch\u0026amp;quot; `$FileScanner.getMask` $FileScanner.getHandle StrCmp $FileScanner.getHandle 0 0 loop MessageBox MB_OK \u0026amp;quot;Error\u0026amp;quot; IDOK close loop: ${locate::Find} $FileScanner.getHandle $FileScanner.getPathName $FileScanner.getPath $FileScanner.getName $FileScanner.getSize $FileScanner.getTime $FileScanner.getAttr /*MessageBox MB_OKCANCEL \u0026#39;$$R1 \u0026amp;quot;pathname\u0026amp;quot; =[$R1]$n $$R2 \u0026amp;quot;path\u0026amp;quot; =[$R2]$n $$R3 \u0026amp;quot;name\u0026amp;quot; =[$R3]$n $$R4 \u0026amp;quot;size\u0026amp;quot; =[$R4]$n $$R5 \u0026amp;quot;time\u0026amp;quot; =[$R5]$n $$R6 \u0026amp;quot;attributes\u0026amp;quot; =[$R6]$n $n\u0026#39;*/ ${Logger.log} \u0026amp;quot;Found : $FileScanner.getPathName\u0026amp;quot; ${If} $FileScanner.getName == \u0026amp;quot;\u0026amp;quot; goto close ${Else} call $FileScanner.callBackAddress ${EndIf} goto loop close: ${locate::Close} $FileScanner.getHandle ${locate::Unload} functionend No debug I hadn’t managed to find a good way of debugging a NSIS script at runtime. I had found a plugin called Dumpstate which was stopping the installer at the breakpoint and showing me the state of the registries and the stack. However, it was very difficult to understand why it was failing at certain points.\nNo OOP In NSIS I could not use the OOP principles. I was not able to create objects. I had to simulate it with macros and separate files that had getters.\n1 2 3 4 5 6 7 8 9 10 11 12 function ActiveMQ.setup ${Logger.log} \u0026amp;quot;Installing ActiveMQ\u0026amp;quot; DetailPrint \u0026amp;quot;Installing ActiveMQ\u0026amp;quot; ${Logger.log} \u0026amp;quot;Copy ActiveMQ file to directory $DirectoryPage.getUserActiveMQDirectory\u0026amp;quot; CopyFiles /SILENT \u0026#39;$EXEDIR${ComponentLocation.activemq}*.*\u0026#39; \u0026#39;$DirectoryPage.getUserActiveMQDirectory\u0026#39; nsExec::ExecToLog \u0026amp;quot;$DirectoryPage.getUserActiveMQDirectorybinwin$System.getSystemBitTypeInstallService.bat\u0026amp;quot; pop $1 ${System.checkScriptStatus} \u0026amp;quot;$DirectoryPage.getUserActiveMQDirectorybinwin$System.getSystemBitTypeInstallService.bat\u0026amp;quot; $1 pop $0 ${Logger.log} \u0026amp;quot;ActiveMQ was installed successfuly\u0026amp;quot; DetailPrint \u0026amp;quot;ActiveMQ was installed successfuly\u0026amp;quot; functionEnd Plugins never work During the development, I was always searching for plugins that would save time and shorten the code. Sadly it wasn’t always the case. Every time I tried a new plugin it was failing. Besides this, the documentation for some of them was missing. Each project has its own .nsis files inside the Java module that has to be compiled via a Maven goal to get the .exe file. So, the pipeline was bounded to the NSIS compiler when we were preparing the install package. This was time consuming and the DEV machine had to have a compiler installed.\nPython NSIS was a mistake, and I admit it. Half a year later, I decided that because I was the one who has created the installer with NSIS and it was so hard to maintain, I should be the one who will change it! It is time to use another language and another approach. First, I thought to remove the pre-processing steps like compiling .nsis files to have the install pack faster. I’ve decided to make the installer as a separate project.\nI’ve decided to build it in Python, I want to learn it. Python had everything I needed:\nGUI libs OOP Easy syntax Widely supported Extensible. The main idea remained the same. The client project has the same file with the dependencies it requires. The only difference is that for now Python has to be installed on the production server to run the installer. How to do that? Simple… self-extracting archive solution provided by 7ZIP. You can configure a .bat file to run after the extraction has been finished. The procedure of creating the install pack also remained the same. Nobody even noticed anything.\nHere is how the new installer looks like: Conclusion Think twice when it comes to choose a development language. I really enjoyed working in Python and building the installer from scratch rather than in NSIS. Having the installer as a modern separate project makes it easy to maintain and improve not only by me, but by other developers as well, who know python.\n","permalink":"https://dcebotarenco.github.io/posts/nsis-vs-python/","summary":"All companies have a delivery pipeline of the product. Every company can create its own pipeline and choose any strategy to deliver. In this post I’m going to talk about NSIS and Python. I had a task to think about the product deployment in the company I’m working for. Before I got this task, we used to copy paste files via RDP and VPN to the client server and install them manually.","title":"🧰 Nsis vs Python"},{"content":"We all tend to do our work faster because time costs money. I realized that we lose a lot of time waiting for IDE to start. I use Netbeans IDE and all our work is related to it. For Java modules, we use Maven and, of course, we have some custom goals. Some goals are pure Java, though we have some goals that use SVN for committing, logging, reverting and so on. And some of them are related to our RDBMS as we load a DB.\nBut the main problem is that we can do all of this only when Netbeans IDE finishes its class path scanning and other indexing background processes. This, depending on your hardware, could sometimes take up to 3-5 minutes. I didn’t like it, so I decided to make this process a bit faster.\nThe following main features we use, which I could improve:\nLoad database SonarQube scanning Clean and build Pull the source code aka Git but this is for SVN Push the source code aka Git but this is for SVN SVN reset SVN log Java Deploy All these features are related to Maven, therefore, we can write Maven commands. Here is where the BASH comes in! We all know that bash is a very popular and flexible scripting language used by many people around the world. I decided to install and use it instead of the default CMD from Windows. However, it is really hard to remember and write a command like:\n1 mvn clean install myPlugin:loaddb myPlugin:execmodule myPlugin:migratedb myPlugin:activemq -Pdevelopment -Dactivemq.purgeAllQueues=true I could have used some plugins from GitHub that provide some features, like fast running a command. But, since I am not a terminal at this moment person, I decided to write my own bash scripts so I could get a taste of it. This would introduce me to the bash world and give me a brief view of its commands. As starting point I installed Git that provides, by default, the Bash terminal and its environment.\nBash aliases Bash terminal can be configured to load a file at startup. This file is called .bash_profile. You can create it in the %USER_PROFILE% directory. In this file, you can write your own bash scripts and they will be available in the terminal at runtime. I added some aliases that are bound to some functions I could run. Example:\n1 2 3 alias r=\u0026#39;r_registerCurrentDirectoryWithAnAllias\u0026#39; alias j=\u0026#39;j_jumpToADirectory\u0026#39; alias m=\u0026#34;maven_command\u0026#34; Each alias value is a Bash function that is responsible for doing something. Taking in consideration that all commands are Maven based, I defined three aliases I would need:\nM – Run a maven command based on a pom.xml file R – Stands for register. I use it to add a directory with a directory alias, list them or remove one J – Jump to a directory using a directory alias Core and customer structure Before we go through the implementation, I have to explain why I defined these specific aliases. Internally, we build a product for clients that hold warehouses. More or less, they all have mostly the same functionality, so we have divided our product in 2 parts:\nThe Core (common functionality) The Client (customer specific functionality) Because of this, we have different versions for these components. The Core is based on branches and tags like 1.0-SNAPSHOT or 1.0.1.\nThe Client component is based on environments:\nDevelopment-SNAPSHOT, where it may have a dependency on 1.0-SNAPSHOT Acceptance-SNAPSHOT Production-SNAPSHOT This structure creates some patterns for accessing specific project folder, like:\n1 2 3 4 5 D\\:ProjectA\\branches\\1.0-SNAPSHOT D\\:ProjectB\\tags\\1.0.2 D\\:ProjectC\\environment\\development D\\:ProjectC\\environment\\acceptance D\\:ProjectC\\environment\\production Commands R – Register Now let’s see what was optimized by using Bash. Using the above mentioned aliases, I can run the commands. As I mentioned r stands for register, which holds different aliases for different paths. Using r I can run:\na – add alias; ls – list aliases; r – remove an alias. Main function:\n1 2 3 4 5 6 function r_registerCurrentDirectoryWithAnAllias() { command=$1 alias=$2 r_processCommand $command $alias } Delegated function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 function r_processCommand() { command=$1 alias=$2 validateParameter $command local isCommandValid=$? if [ $isCommandValid == 0 ] then case $command in \u0026#34;a\u0026#34; ) validateParameter $alias local isAlliasValid=$? if [ $isAlliasValid == 0 ] then add $alias else echo \u0026#34;Missing alias\u0026#34; fi ;; \u0026#34;ls\u0026#34; ) initFile out_pathDir pathDir=$out_pathDir fileLines $pathDir 0 ;; \u0026#34;r\u0026#34; ) validateParameter $alias local isAlliasValid=$? if [ $isAlliasValid == 0 ] then remove $alias else echo \u0026#34;Missing alias\u0026#34; fi ;; esac else echo \u0026#34;Missing command\u0026#34; fi } Now I go to the pom.xml file of the project and run: r a test – which means - register add for the current path the alias test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 function add() { initFile out_pathDir pathDir=$out_pathDir alias=$1 currentDir=$(pwd) fileLines $pathDir 1 file_lines_array_size=${#fileLines_lines[@]} if [ $file_lines_array_size == 0 ] then echo \u0026#34;$alias=$currentDir\u0026#34; \u0026amp;gt;\u0026amp;gt; $pathDir else array_of_lines=${fileLines_lines[@]} for line in $array_of_lines do lineKeyValue $line out_key out_value if [ $out_value == $currentDir ] then echo \u0026#34;Such directory already exists on $line\u0026#34; break else echo \u0026#34;$alias=$currentDir\u0026#34; \u0026amp;gt;\u0026amp;gt; $pathDir break fi done fi } This command creates in background a file .pathDir which will be used as a storage for all the aliases. Now I have an alias inside it.\nIn order to see what other aliases I have, I can write: r ls – this command will display all the aliases I have in the .pathDir\nAnd of course, I can remove an alias by writing: r r test – which means: register remove the alias test linked to any path. Now I can use test for any other path from the system.\n1 2 3 4 5 6 7 8 9 function remove() { initFile out_pathDir pathDir=$out_pathDir alias=$1 currentDir=$(pwd) sed \u0026#34;/$alias=/d\u0026#34; $pathDir \u0026gt;\u0026gt; \u0026#34;$pathDir tmp\u0026#34; \u0026amp;\u0026amp; mv \u0026#34;$pathDir tmp\u0026#34; $pathDir } J – Jump After we added an alias for a path, we can now directly jump to that directory by typing: j iut – it’s a simple cd to the referenced path.\nMain function:\n1 2 3 4 5 6 function j_jumpToADirectory { alias=$1 branch=$2 j_jump $alias $branch } However, this does not bring you a lot of magic since we might have tags, branches and environments for a project. Well, jump supports additional parameters to the command, like: j iut 1.0.2 – this way we can jump specifically inside the needed folder. But in this case iut alias should be linked to ‘D:\\ProjectA\\’ , because this will be the root folder of the project. We can also jump to a tag or an environment if it exists, by writing the following command: j iut 1.0.2 or j iut d where d stands for development environment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 function j_jump { local alias=$1 local branch=$2 validateParameter $alias local isAliasValid=$? if [ $isAliasValid == 0 ] then initFile out_pathDir pathDir=$out_pathDir fileLines $pathDir 1 file_lines_array_size=${#fileLines_lines[@]} if [ $file_lines_array_size != 0 ] then array_of_lines=${fileLines_lines[@]} local isFound=1 for line in $array_of_lines do lineKeyValue $line out_key out_value if [ $out_key == $alias ] then isFound=0 local path=$out_value validateParameter $branch local isBranchValid=$? if [ $isBranchValid != 0 ] then cd $path break else cd $path j_process_branch $branch out_path local fullpath=\u0026#34;$path/$out_path\u0026#34; cd $fullpath break fi fi done if [ $isFound == 1 ] then echo \u0026#34;No such alias found\u0026#34; fi else echo \u0026#34;No aliases found\u0026#34; fi else echo \u0026#34;Missing parameter\u0026#34; fi } function j_process_branch { local branch=$1 local pth=$2 isMajorMinorFunc $branch local isMajorMinor=$? isTagFunc $branch local isTag=$? isPrivateNumberFunc $branch local isPrivateBranch=$? if [ $isMajorMinor == 0 ] then eval $pth=\u0026#34;branches/$branch-SNAPSHOT\u0026#34; elif [ $isTag == 0 ] then eval $pth=\u0026#34;tags/$branch\u0026#34; elif [ $isPrivateBranch == 0 ] then eval $pth=\u0026#34;branches//private-$branch\u0026#34; else case $branch in \u0026#34;t\u0026#34; ) eval $pth=\u0026#34;branches/TRUNK-SNAPSHOT\u0026#34; ;; \u0026#34;d\u0026#34; ) eval $pth=\u0026#34;environment/development\u0026#34; ;; \u0026#34;a\u0026#34; ) eval $pth=\u0026#34;environment/acceptance\u0026#34; ;; \u0026#34;p\u0026#34; ) eval $pth=\u0026#34;environment/production\u0026#34; ;; esac fi } M – Maven Now, when we are able to jump faster from one project to another, we can run maven goals over the pom.xml files, like this:\nm ldb – loads database m pull – SVN update goal m push – SVN commit goal m reset – SVN revert goal m log – SVN log goal m deploy – deploy m sonar – sonar goal 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 function maven_command { local command=$1 validateParameter $command local isCommandValid=$? if [ $isCommandValid == 0 ] then case $command in \u0026#34;sonar\u0026#34; ) mvn clean install sonar:sonar -Dsonar.host.url=\u0026#34;https://sonarqube-host:9000\u0026#34; ;; \u0026#34;ldb\u0026#34; ) local setup=$(find -regex \u0026#34;.*setup$\u0026#34; | head -1) cd \u0026#34;$setup\u0026#34; mvn clean install myPlugin:loaddb myPlugin:execmodule myPlugin:migratedb myPlugin:activemq -Pdevelopment -Dactivemq.purgeAllQueues=true ;; \u0026#34;ci\u0026#34; ) mvn clean install ;; \u0026#34;pull\u0026#34; ) mvn myPlugin:ts-update -N ;; \u0026#34;push\u0026#34; ) mvn myPlugin:ts-commit -N ;; \u0026#34;reset\u0026#34; ) mvn myPlugin:ts-revert -N ;; \u0026#34;log\u0026#34; ) mvn myPlugin:ts-log -N ;; \u0026#34;deploy\u0026#34; ) mvn clean source:jar javadoc:jar deploy -Pjavadocprof ;; *) mvn $@ ;; esac else echo \u0026#34;Missing command\u0026#34; fi } Conclusion Having all of this configured, we can run the load the database or any command over a pom.xml file without even having to start the Netbeans IDE. The .bash_profile can be customized per developer, so everyone can add commands of their own. The most important thing is that using Bash we can gain a lot of power due to its internal commands that Windows does not have. We save a lot of time. And while Netbeans starts, you could already have 3-4 terminals working:\nOne loading the DB Another loading logs Third updating another project ","permalink":"https://dcebotarenco.github.io/posts/power-of-bash/","summary":"We all tend to do our work faster because time costs money. I realized that we lose a lot of time waiting for IDE to start. I use Netbeans IDE and all our work is related to it. For Java modules, we use Maven and, of course, we have some custom goals. Some goals are pure Java, though we have some goals that use SVN for committing, logging, reverting and so on.","title":"💪 Power of Bash"},{"content":"Intro Once I read a post about a guy who was searching for an apartment in San Francisco. He was annoyed about the fact that searching for “How to find an apartment in San Francisco” on Google yields dozens of pages of advice as a good indicator of apartment hunting and this is a painful process.\nSo he decided to implement a bot in Slack that would help him to search an apartment based on scraping data from websites and notifying him about the cheapest and closest apartment around him. His idea inspired me to create a bot, although I didn’t know at that time what are the functionalities I am going to build.\nThe idea We started brainstorming, suddenly in the Skype chat I saw “Don’t forget to order food”. Let me explain, internally we ordered food from a local catering company and our office manager each morning was pasting this message. We have a google sheet where we store our choices. Later, based on choices, the office manager places an order. That is it! We need a bot that would order food for us. A lad charea joined me so we could do it twice as fast (we thought).\nChoosing the right tools Together we started to investigate on what platforms a bot can be built and we found a quite new and fresh platform, that is developed by Microsoft, called Microsoft Bot framework. We have found it very interesting and powerful. The only thing we had to do is to register on their platform. They provide an API in NodeJS and C#. We haven’t spent a lot of time choosing. Since we do Java and C# is almost the same, we decided to go for NodeJS because it was something new for us. If we had chosen C#, then the development of the bot would not have been so funny and challenging.\nMicrosoft Bot framework It enables organizations to build intelligent agents, known as Bots. The Framework provides developers with a developer portal and SDK to build Bots, a Bot Connector service to connect to social channels such as Twitter and Slack, and a Bot Directory (Marketplace) to discover and use existing bots It supports Node JS and C#. It provides a powerful session management and some interactive cards like Hero, Thumbnail, Receipt and Sign-In. NodeJS \u0026amp; ES6 We will not speak a lot about NodeJS, but there is something to mention:\nAn important thing to realize is that Node is not a web server. By itself it doesn’t do anything. It doesn’t work like Apache. There is no config file where you point it to your HTML files. If you want it to be a HTTP server, you have to write an HTTP server (with the help of its built-in libraries).\\ Node.js is just another way to execute code on your computer. It is simply a JavaScript run time; ECMAScript 6 is a new version of ECMAScript providing a new set of features and fixes to JavaScript. IDE This was another tool we had to decide on. There aren’t a lot of free good IDEs that would support NodeJS. Most popular found:\nWebstorm (30 days) Netbeans 8.2 (free) Eclipse (using some plugins) Sublime Atom We went for Webstorm and Netbeans. Btw Webstorm is the best. Bot Framework simulator Another tool we have had is the simulator provided by the platform. This tool is simulating a REST server with which our bot communicates using the predefined API. It is like a local Skype server. This gives you the possibility to chat with your bot locally without deploying it. Using the Emulator, you can:\nSend requests and receive responses to/from your bot endpoint on localhost; Inspect the JSON response; Emulate a specific user and/or conversation. Google API In order to be able to access Google Spreadsheets we needed to find a way of interacting with our application, and luckily for us, Google officially supported a Node.js client library for accessing Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT is included. At this point we had to choose which type of authentication fits best for us. Since we needed a type of authentication in which the end-user would not be involved in the process of generating tokens to establish the identity within Google API, we had chosen JWT(JSON Web Token).\nTokens are generated for a service account, which is created from the Google API console. Service accounts must also be granted access to resources, using traditional assignation of permissions using the unique service account email address. The application requests a token that can be used for authentication in exchange with a valid JWT. The resulting token can then be used for multiple API calls, until it expires and a new token must be obtained by submitting another JWT.\nThe Bot Mainly our bot gathers user choices regarding lunch options and places orders in the Google Sheet. To use the bot it is necessary to install it via a Skype, join URL and register yourself. The registration itself is implemented in a very simple way. We have two locked columns in the Google Sheet. First column is the ID and the second one is the Skype name. For each Skype name we have a specific ID. This ID is provided by the Skype service. After the user completes the registration, he/she is able to use the bot. The bot provides some dialogs that help the user choosing his/her meal:\nInteractive Dialogs: Greeting Dialog Help Place Order Cancel Orders Status Non interactive Dialogs: Jokes Let’s have a closer look at them. Greeting Dialog This dialog provides a greeting message together with the user ID that needs to be sent to the administrator. The trigger keyword is ‘hi’. Help Dialog Dialog that shows to the user all commands that can be entered. The trigger keyword is ‘help’. Place Order Dialogs If the user is hungry then he/she has to place the order by writing ‘food’ The Bot will show the menu for the current day. If the user wants to order food for another day then the command should be like : ‘food (today|mo|tu|we|th|fr)’ It is possible to order more than 1 meal per day. Number of orders per one day is equal with the number of rows related to the user in the Google Sheet If the user has 1 meal and it’s completed it will be overridden If the user has 2 or more meals and these are completed, the bot will ask him to cancel one of them This Dialog is triggered by a cron expression that is running in background Cancel Order Dialog In case the user decided to cancel an order, he/she has to write: ‘food cancel (today|mo|tu|we|th|fr)’ by specifying the day for which the he/she wants to cancel the order If the user has more than 1 meals on that day, the bot will ask him to cancel 1 meal Status Dialog In order to see what is the current choice, the user can write ‘food status’ The Bot will show the user’s today choices If the user wants to see the food status for another day then the ‘food status (today|mo|tu|we|th|fr)’ command can be used. Heroku integration and hot deploy You may ask yourself where do we run it. We have chosen the PaaS platform HEROKU that is very easy to use:\nHeroku is a cloud platform that lets companies build, deliver, monitor and scale apps — we’re the fastest way to go from idea to URL, bypassing all those infrastructure headaches; Heroku is a cloud platform as a service(PaaS). That means you do not have to worry about infrastructure, you just focus on your application.\nHere are some features of Heroku Instant Deployment with Git push – the build of your application is performed by Heroku using your build scripts Plenty of Add-on resources (applications, databases etc.) Processes scaling – independent scaling for each component of your app without affecting functionality and performance Isolation – each process (aka dyno) is completely isolated from each other; Full Logging and Visibility – easy access to all logging output from every component of your app and each process (dyno); Heroku provides very well written tutorial which allows you to start in minutes. Also they provide first 750 computation hours free of charge which means you can have one processes (aka Dyno) at no cost. Also performance is very good e.g.a simple web application written in Node.js can handle around 60 – 70 requests per second. Publishing the bot Publishing the bot basically means to add it to the Bot Directory which is a public directory of all bots that were registered and published within the Microsoft Bot Framework. Users can select your bot in the directory and add it to one or more of the configured channels that they use. After you publish your bot, Microsoft will review your bot submission to make sure it meets a certain minimum requirements before it is publicly available on the Bot Directory. Although, the bot can be used without being published, there is a limit of 100 users.\n","permalink":"https://dcebotarenco.github.io/posts/skype-bot/","summary":"Intro Once I read a post about a guy who was searching for an apartment in San Francisco. He was annoyed about the fact that searching for “How to find an apartment in San Francisco” on Google yields dozens of pages of advice as a good indicator of apartment hunting and this is a painful process.\nSo he decided to implement a bot in Slack that would help him to search an apartment based on scraping data from websites and notifying him about the cheapest and closest apartment around him.","title":"🤖 Skype bot"},{"content":"Intro 💁 I\u0026rsquo;m Dan. Code for food 10+ years. Mainly focused on Java, JVM, frameworks and languages around it. Practicing Extreme Programming and clean code. I like table tennis, volleyball and books\n","permalink":"https://dcebotarenco.github.io/about/","summary":"Intro 💁 I\u0026rsquo;m Dan. Code for food 10+ years. Mainly focused on Java, JVM, frameworks and languages around it. Practicing Extreme Programming and clean code. I like table tennis, volleyball and books","title":""}]