[{"content":"Build a Spring-boot Hazelcast cluster in Kubernetes For local testing purposes, you might want to have a cluster of microservices that use Hazelcast where you can watch the replication, rollout of pods and test some Kubernetes infrastructure-related changes.\nPrerequisites Docker minikube kubectl helm minikube Dashboard or Lens Goal Deploy a spring-boot hazelcast cluster locally using helm and connect to it via hazelcast management center Isolate the POC in a separate environment and play with the same microservices and Hazelcast upgrades Limitations We cannot push to any cloud image registries like Azure Container Registry, say it\u0026rsquo;s forbidden by the security policy.\nLocal Hazelcast cluster setup Create a spring boot microservice let\u0026rsquo;s call it my-service\nCode configuration Hazelcast uses by default cluster name dev. For the POC we will name it hazelcast-cluster. We also provide the cluster DNS name hazelcast-dns.default.svc.cluster.local. You can externalize it in a property as well. I also changed the default port to 5702 for sake of the demo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Bean public Config config(HazelcastSettings hazelcastSettings) { Config config = new Config(); config.setInstanceName(\u0026#34;my-service\u0026#34;); config.setClusterName(\u0026#34;hazelcast-cluster\u0026#34;); config.getMapConfig(\u0026#34;my-cache\u0026#34;).setInMemoryFormat(InMemoryFormat.OBJECT); JoinConfig join = config.getNetworkConfig() .setPort(5702) .getJoin(); join.getMulticastConfig().setEnabled(false); join.getKubernetesConfig() .setEnabled(true) .setProperty(SERVICE_DNS_PROPERTY, \u0026#34;hazelcast-dns.default.svc.cluster.local\u0026#34;) .setProperty(DNS_TIMEOUT_PROPERTY, \u0026#34;5\u0026#34;); return config; } @Bean(name = \u0026#34;cacheInstance\u0026#34;) public HazelcastInstance hazelcastInstance(Config config) { return Hazelcast.getOrCreateHazelcastInstance(config); } Build the jar This part is easy.\n1 gradle clean bootJar Test your jar 1 2 3 4 java -Dspring.profiles.active=dev \\ -DJASYPT_KEY=jasyptCode \\ -Dspring.config.additional-location=file:application/src/main/resources/application-dev.properties \\ -jar application/build/libs/my-service-0.0.0-local.jar Build the image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 FROM openjdk:11-jre-slim ARG APPLICATION_ROOT=/application ARG LOG_DIR=${APPLICATION_ROOT}/logs RUN apt update \u0026amp;\u0026amp; apt install -y dnsutils RUN apt update \u0026amp;\u0026amp; apt install -y iputils-ping RUN mkdir -p ${LOG_DIR} \u0026amp;\u0026amp; \\ chown 1000:1000 ${LOG_DIR} \u0026amp;\u0026amp; \\ chown 1000:1000 ${APPLICATION_ROOT} COPY *.jar /application/app.jar COPY application.properties /application/application.properties RUN chmod 0755 /application \u0026amp;\u0026amp; \\ chmod 0444 /application/app.jar USER 1000:1000 WORKDIR /application ENTRYPOINT [ \u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/application/app.jar\u0026#34; ] EXPOSE 8081 Move the files to the /lib folder, then run from the project root folder:\n1 2 3 docker build --no-cache --progress=plain \\ -t my-service:1.0 \\ -f platform/docker/Dockerfile application/build/libs Run the container:\n1 2 3 4 docker run -p 8080:8081 \\ -e JASYPT_KEY=\u0026#39;xyzZXy123\u0026#39; \\ my-service:1.0 \\ --spring.config.additional-location=file:/application/application.properties You can also mount a volume to not copy everything inside the container 1 2 3 4 5 docker run -p 8080:8081 \\ -e JASYPT_KEY=\u0026#39;jasyptCode\u0026#39; \\ -v \u0026#34;/$(pwd)/application/src/main/resources/application.properties:/application/application.properties\u0026#34; \\ my-service:1.0 \\ ‚îÄ --spring.config.additional-location=file:/application/application.properties Check the image You should see the built image my-service:1.0\n1 docker images Start Minikube I assume you have minikube installed.\n1 2 3 minikube start minikube addons enable ingress #for ingress minikube addons enable registry #for registry Load image in minikube 1 minikube image load my-service:1.0 Keep in mind you have 2 docker engines, the local and the one inside minikube, this is why we need to pull and load(upload) images to minikube\u0026rsquo;s docker. You can avoid this by pointing the docker CLI to the minikube\u0026rsquo;s docker and just build and pull right inside the minikube.\n1 2 eval $(minikube -p minikube docker-env) //point docker to internal minikube docker eval $(minikube docker-env --unset) //point docker back to your local docker Helm Chart Chart.yaml\n1 2 3 4 apiVersion: v1 description: A Helm chart for Kubernetes for service my-service name: my-service version: 0.1.0 Some values.yaml\n1 2 3 4 5 6 7 8 9 environment: \u0026#34;local\u0026#34; replicaCount: \u0026#34;2\u0026#34; image: repository: \u0026#34;my-service\u0026#34; // when on local use only the image name tag: \u0026#34;1.0\u0026#34; pullPolicy: \u0026#34;Never\u0026#34; // do not pull it from any repo, load from minikube internals service: externalPort: \u0026#34;8081\u0026#34; internalPort: \u0026#34;8081\u0026#34; The placeholders are automatically resolved by Helm\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name }} labels: app: {{ .Chart.Name }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app: {{ .Chart.Name }} template: metadata: labels: app: {{ .Chart.Name }} spec: securityContext: runAsUser: 1000 containers: - name: {{ .Chart.Name }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}\u0026#34; args: [ \u0026#34;--spring.config.additional-location=file:/application/application.properties\u0026#34; ] imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: {{ .Values.service.internalPort }} env: - name: JAVA_TOOL_OPTIONS value: -Xms512m -XX:MaxRAMPercentage=75.0 -XX:MaxMetaspaceSize=300m -XX:+PrintFlagsFinal -XshowSettings:vm - name: ENVIRONMENT value: {{ .Values.environment }} - name: JASYPT_KEY value: anotherKey livenessProbe: httpGet: path: /actuator/health/liveness port: {{ .Values.service.internalPort }} scheme: HTTP initialDelaySeconds: 120 periodSeconds: 20 timeoutSeconds: 6 readinessProbe: httpGet: path: /actuator/health/readiness port: {{ .Values.service.internalPort }} scheme: HTTP initialDelaySeconds: 120 periodSeconds: 30 timeoutSeconds: 6 ingress.yaml. Open access to service under a certain path using rewrite-target, regex group $2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: rules: - host: my-host - http: paths: - path: /mega(/|$)(.*) pathType: Prefix backend: service: name: my-service port: number: 8081 Publish docker image Usually, Helm will fetch it from a container registry. Be we cannot. If we deploy now the Chart, Helm won\u0026rsquo;t see the docker image We have 2 options:\nThe easy way. Upload the image manually from the host into minikube The hard way. Create a registry. Details here: Registry Docker Registry Deploy Helm Chart in minikube Some Helm commands that you might want to use\n1 2 3 4 helm install hazelcast-poc platform/helm/my-service helm upgrade hazelcast-poc platform/helm/my-service helm rollback hazelcast-poc helm uninstall hazelcast-poc Shell into one pod and test the DNS. You should see your 2 pods\n1 2 3 4 5 6 7 8 9 10 I have no name!@my-service-679d747874-vklld:/application$ nslookup hazelcast-dns Server: 10.96.0.10 Address: 10.96.0.10#53 Name: hazelcast-dns.default.svc.cluster.local Address: 172.17.0.7 Name: hazelcast-dns.default.svc.cluster.local Address: 172.17.0.6 I have no name!@my-service-679d747874-vklld:/application$ Deploy hazelcast management center 1 minikube image pull hazelcast/management-center Launch the pod\n1 kubectl create deployment hazelcast-center --image=hazelcast/management-center:latest Forward the port 1 2 kubectl get pods kubectl port-forward hazelcast-center-6f9b687779-krvl7 8080:8080 Open management center in browser localhost:8080. If you don\u0026rsquo;t see both members in the cluster, you can downscale the pods, they will appear. Ingress We can also access our service via ingress on path localhost/mega. Before that enable minikube ingress tunneling:\n1 minikube tunnel 1 2 curl http://localhost/mega/actuator/loggers/root # {\u0026#34;effectiveLevel\u0026#34;:\u0026#34;INFO\u0026#34;} Play with the incremental Hazelcast upgrade by building new images and upgrading the Helm chart. Happy Helming\n","permalink":"https://dcebotarenco.github.io/posts/hazelcast-in-minikube/","summary":"Build a Spring-boot Hazelcast cluster in Kubernetes For local testing purposes, you might want to have a cluster of microservices that use Hazelcast where you can watch the replication, rollout of pods and test some Kubernetes infrastructure-related changes.\nPrerequisites Docker minikube kubectl helm minikube Dashboard or Lens Goal Deploy a spring-boot hazelcast cluster locally using helm and connect to it via hazelcast management center Isolate the POC in a separate environment and play with the same microservices and Hazelcast upgrades Limitations We cannot push to any cloud image registries like Azure Container Registry, say it\u0026rsquo;s forbidden by the security policy.","title":"‚õÖ Hazelcast in minikube"},{"content":"üßë‚Äçüéì tltr 1 1234 Print is executed multiple times till the first value that matches both filter conditions is found.\n‚ùìQuestion 1 2 3 4 5 6 //What is the output? IntStream.range(1, 10) .peek(System.out::print) .filter(i -\u0026gt; i \u0026gt; 2) //A .filter(i -\u0026gt; i \u0026gt; 3) //B .findFirst(); üïµÔ∏è‚Äç‚ôÇÔ∏è Explanation Let\u0026rsquo;s simplify it\n1 IntStream.range(1, 10); This won\u0026rsquo;t do anything because there is no terminal operation which triggers the reading from the source stream.\nBy adding the peek, which is an intermediate operation, the behaviour won\u0026rsquo;t change.\n1 IntStream.range(1, 10).peek(System.out::print); This findFirst (terminal operation) evaluates the pipeline.\n1 IntStream.range(1, 10).peek(System.out::print).findFirst(); During the evaluation, the sinks are wrapped (combined) into one Sink. As a result, we get:\npeek Sink filter A Sink filter B Sink findFirst Sink While the sink is not cancelled (in our case findFirst has no value) and there is an element in the stream, the pipeline will try to advance, fetch the next element and pass it to the wrapped sink as above (top-down)\n‚úÖ Walkthrough So we will have something like this (since the stream is not empty):\nThe stream: 1 2 3 4 5 6 7 8 9\n1 is passed to peek sink peek is executed, 1 is printed 1 passed to filter A sink filter A is executed, false is returned (1\u0026gt;2), the sink is not cancelled, continue the while 2 is fetched and passed to peek sink peek is executed, 2 is printed 2 passed to filter A sink filter A is executed, false is returned (2\u0026gt;2), the sink is not cancelled, continue the while 3 is fetched and passed to peek sink peek is executed, 3 is printed 3 passed to filter A sink filter A is executed, true is returned (3\u0026gt;2) 3 passed tofilter B sink filter B is executed, false is returned (3\u0026gt;3), the sink is not cancelled, continue the while 4 is fetched and passed to peek sink peek is executed, 4 is printed 4 passed to filter A sink filter A is executed, true is returned (4\u0026gt;2) 4 passed to filter B sink filter B is executed, true is returned (4\u0026gt;3) 4 passed to findFirst sink saves the value inside the findFirst which means the sink is cancelled, while interrupted 4 returned Another Example Let\u0026rsquo;s move the peek before the findFirst. Almost the same code, and different output\n1 2 3 4 5 6 //What is the output? IntStream.range(1, 10) .filter(i -\u0026gt; i \u0026gt; 2) //A .filter(i -\u0026gt; i \u0026gt; 3) //B .peek(System.out::print) .findFirst(); Output:\n1 4 The sink:\nfilter A Sink filter B Sink peek Sink findFirst Sink ‚úÖ Walkthrough So we will have something like this: The stream: 1 2 3 4 5 6 7 8 9\n1 is passed to filter A sink filter A is executed, false is returned (1\u0026gt;2), the sink is not cancelled, continue the while 2 is fetched and passed to filter A sink filter A is executed, false is returned (2\u0026gt;2), the sink is not cancelled, continue the while 3 is fetched and passed to filter A sink filter A is executed, true is returned (3\u0026gt;2) 3 passed tofilter B sink filter B is executed, false is returned (3\u0026gt;3), the sink is not cancelled, continue the while 4 is fetched and passed to filter A sink filter A is executed, true is returned (4\u0026gt;2) 4 passed tofilter B sink filter B is executed, true is returned (4\u0026gt;3) 4 is passed to peek sink 4 is printed 4 is passed to findFirst sink saves the value inside the findFirst which means the sink is cancelled, while interrupted 4 returned üìö Read more Sink definition Stream API documentation\n","permalink":"https://dcebotarenco.github.io/posts/stream-api-question/","summary":"üßë‚Äçüéì tltr 1 1234 Print is executed multiple times till the first value that matches both filter conditions is found.\n‚ùìQuestion 1 2 3 4 5 6 //What is the output? IntStream.range(1, 10) .peek(System.out::print) .filter(i -\u0026gt; i \u0026gt; 2) //A .filter(i -\u0026gt; i \u0026gt; 3) //B .findFirst(); üïµÔ∏è‚Äç‚ôÇÔ∏è Explanation Let\u0026rsquo;s simplify it\n1 IntStream.range(1, 10); This won\u0026rsquo;t do anything because there is no terminal operation which triggers the reading from the source stream.","title":"ü™§ Peek \u0026 Filter Stream API"},{"content":"üßë‚Äçüéì tltr Infinite while loop was causing the issue. Write unit tests. Test the exit condition of your while loops.\nüêõ Issue We had some kind of serious performance issue on production. The CPU periodically was pegged at 100% and stayed that way until we restarted the Tomcat. It was degrading the overall site performance for quite a while. We had two instances with peak similarly in CPU. Seems like it must be something inside the Tomcat since that\u0026rsquo;s what is eating the CPU.\nHere is the 6-week view: Here is the 12-week view: The immediate fall of the CPU is where we restart the Tomcat. As you see, after the restart, the CPU jumps back on the ladder ü™ú in a couple of days.\nüïµÔ∏è‚Äç‚ôÇÔ∏è Investigation 1. Thread dump \u0026amp; top -H On Linux, the top -h \u0026lt;pid\u0026gt; is useful to determine the overall usage by all processes and break it down to certain threads within a process. Our PID: Run kill 3 \u0026lt;pid\u0026gt; on the main PID that\u0026rsquo;s consuming a lot of CPU. This will give you the thread dump. It does not stop the running process, but it stops the world inside the JVM.\nWhat we see is 596 of CPU%. This is related to the number of cores attached to the machine. One core is 100% Convert the PID from top -H from decimal to hex and match it with the nid from the thread dump.\n1 6458 -\u0026gt; nid=0x1936 Not the ideal way to monitor a JVM instance. You should prefer agents like Sematext, JProfiler, YourKit or JFR. But we have what we have üëâ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026#34;http-nio-8080-exec-127\u0026#34; #328 daemon prio=5 os_prio=0 tid=0x00007f9f20020000 lwp=6454 nid=0x1936 runnable [0x00007f9e4ada4000] java.lang.Thread.State: RUNNABLE at java.util.HashMap.hash(HashMap.java:339) at java.util.HashMap.put(HashMap.java:612) at java.util.HashSet.add(HashSet.java:220) at java_util_Set$add$0.call(Unknown Source) at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) \u0026#34;http-nio-8080-exec-156\u0026#34; #39893 daemon prio=5 os_prio=0 tid=0x00007f9f20024000 lwp=18378 nid=0x47ca runnable [0x00007f9e3199e000] java.lang.Thread.State: RUNNABLE at java.util.HashMap.hash(HashMap.java:339) at java.util.HashMap.put(HashMap.java:612) at java.util.HashSet.add(HashSet.java:220) at java_util_Set$add$0.call(Unknown Source) at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) \u0026#34;http-nio-8080-exec-95\u0026#34; #203 daemon prio=5 os_prio=0 tid=0x00007f9f200a9000 lwp=5605 nid=0x15e5 runnable [0x00007f9e48d82000] java.lang.Thread.State: RUNNABLE at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) at com.site.api.Controller$$FastClassBySpringCGLIB$$8fdc140f.invoke(\u0026lt;generated\u0026gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:752) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) \u0026#34;http-nio-8080-exec-120\u0026#34; #321 daemon prio=5 os_prio=0 tid=0x00007f9f20036800 lwp=6351 nid=0x18cf runnable [0x00007f9e4cec3000] java.lang.Thread.State: RUNNABLE at java_util_Set$add$0.call(Unknown Source) at com.site.api.Controller.method(Controller.groovy:95) at com.site.api.Controller.method(Controller.groovy) at com.site.api.Controller$$FastClassBySpringCGLIB$$8fdc140f.invoke(\u0026lt;generated\u0026gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:752) 2. Traffic I analyzed the traffic at the point of time where the CPU jumps. I got some httpd logs. httpd is in front of Tomcat for PerimeterX compatibility. Example of logs:\n1 2 87.250.224.200 - - [06/Jun/2022:21:17:08 +0000] \u0026#34;GET /the-faulty-endpoint/id-1 HTTP/1.1\u0026#34; 502 341 \u0026#34;https://www.site.com/pages/page-1\u0026#34; \u0026#34;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.268\u0026#34; 120120565 85.45.207.139 - - [06/Jun/2022:21:18:08 +0000] \u0026#34;GET /the-faulty-endpoint/id-2 HTTP/1.1\u0026#34; 502 341 \u0026#34;https://www.site.com/pages/page-1\u0026#34; \u0026#34;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.268\u0026#34; 120122901 At that point in point in time, I saw just a lot of bots (Apple, Google, Yandex, Bing) indexing the pages\n‚úÖ Put things together What we have now is the requests which tell us the URLs that are called and the values in a URLs. The values in the paths are different id-1, id-2, id-n. We also have the code, which we can analyze locally. Code has no tests üòí\nSimplified pseudo-code:\n1 2 3 4 5 6 7 8 int rowsInDB = findBy(\u0026#39;id-from-url\u0026#39;); int buffer = 10; int minRows = Math.min(rowsInDB, rowsInDB + 10); Set randomIndexes = []; while (randomIndexes.size() \u0026lt; buffer) { randomIndexes.add(random.nextInt(minRows)); } At first, I could not spot the bug because I was on the local environment with no data in my H2 database.\nI assembled a list of all the requests within CPU increase timeframe into a CSV file. Launched locally the JMeter, VisualVM, JDK Mission Control and the app connected to the QA database with data and fired all the requests to my local instance. Finally, I noticed locally a CPU increase, which is not going down.\nThere was an id that returned 5 rows from the database. Then I realized that the problem is related to the random.nextInt(minRows) and the Set collection. Here is why:\n1 2 3 4 5 6 7 8 int rowsInDB = findBy(\u0026#39;faulty-id\u0026#39;); //returns 5 int buffer = 10; int minRows = Math.min(rowsInDB, rowsInDB + 10); //here we get 5 Set randomIndexes = []; while (randomIndexes.size() \u0026lt; buffer) { randomIndexes.add(random.nextInt(minRows)); //seed is 5 } The Set will not increase its size because it accepts unique values that are always between 0-4. Set can contain only 0,1,2,3,4. The size of the set will not increase more than 5 where the buffer is always 10 - infinite loop detected.\nWrote a unit test, confirmed the bug, fixed the test, made a hotfix.\nüìö Userfull links How to get thread id and stack IBM best practices Top -H Fastthread.io\n","permalink":"https://dcebotarenco.github.io/posts/high-cpu-usage-while-loop/","summary":"üßë‚Äçüéì tltr Infinite while loop was causing the issue. Write unit tests. Test the exit condition of your while loops.\nüêõ Issue We had some kind of serious performance issue on production. The CPU periodically was pegged at 100% and stayed that way until we restarted the Tomcat. It was degrading the overall site performance for quite a while. We had two instances with peak similarly in CPU. Seems like it must be something inside the Tomcat since that\u0026rsquo;s what is eating the CPU.","title":"üî• High CPU usage on faulty endpoint"},{"content":"üßë‚Äçüéì tltr Configure evictInBackground, maxIdleTime and maxLifeTime to clear connections from the connection pool or retry the call or use RestTemplate\nüêõ Issue At one of my clients, it was decided no more RestTemplate, all move to üçÉ Spring WebClient. I consider this project very interesting, but it comes with a different mindset which developers do not pay attention to:\nNon-blocking Event driven Connection pool Back to the client. After each deployment, in a matter of days, we were getting - the connection reset by the peer. The server on the other side was resetting the connection. Our connection is not valid anymore. Here is the exception example:\n1 2 3 4 reactor.core.Exceptions$ReactiveException: io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer at reactor.core.Exceptions.propagate(Exceptions.java:392) at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97) at reactor.core.publisher.Mono.block(Mono.java:1706) Logs IPs are cleared.\n1 2 3 4 5 6 7 8 9 10 11 ./catalina.out-20211223.gz:2021-12-22 19:04:34.458 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36] REGISTERED ./catalina.out-20211223.gz:2021-12-22 19:04:34.460 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36] CONNECT: rest.apisandbox.server.com/1.1.1.1:443 ...time... ./catalina.out-20211224.gz:2021-12-23 08:26:16.138 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] READ COMPLETE ./catalina.out-20211224.gz:2021-12-23 08:26:16.138 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] EXCEPTION: io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 WARN --- [or-http-epoll-3] r.netty.http.client.HttpClientConnect : [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] The connection observed an error, the request cannot be retried as the headers/body were sent ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] USER_EVENT: SslCloseCompletionEvent(java.nio.channels.ClosedChannelException) ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] INACTIVE ./catalina.out-20211224.gz:2021-12-23 08:26:16.139 DEBUG --- [or-http-epoll-3] reactor.netty.http.client.HttpClient : [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] UNREGISTERED üïµÔ∏è‚Äç‚ôÇÔ∏è Investigation We can confirm the connection is broken based on the symbols between the IPs:\n1 2 3 4 //Good connection [30857d36-92, L:/0.0.0.0:55868 - R:rest.apisandbox.server.com/1.1.1.1:443] //Bad connection [30857d36-92, L:/0.0.0.0:55868 ! R:rest.apisandbox.server.com/1.1.1.1:443] The legend - means the connection is OK ! means the connection is broken 30857d36 the connection id 92 number of re-usages of the connection Example 1 2 3 2021-12-23 08:26:23.069 DEBUG --- [or-http-epoll-2] reactor.netty.http.client.HttpClient : [338e870a-97, L:/0.0.0.0:55864 ! R:rest.apisandbox.server.com/1.1.1.1:443] USER_EVENT: SslCloseCompletionEvent(java.nio.channels.ClosedChannelException) 2021-12-23 08:26:23.069 DEBUG --- [or-http-epoll-2] reactor.netty.http.client.HttpClient : [338e870a-97, L:/0.0.0.0:55864 ! R:rest.apisandbox.server.com/1.1.1.1:443] INACTIVE 2021-12-23 08:26:23.069 DEBUG --- [or-http-epoll-2] reactor.netty.http.client.HttpClient : [338e870a-97, L:/0.0.0.0:55864 ! R:rest.apisandbox.server.com/1.1.1.1:443] UNREGISTERED The problem The connection was registered/created on: 2021-12-22 19:04:34.458 and it was invalidated on 2021-12-23 08:26:16.139. It means it stayed in the connection pool and was re-used for almost 13 hours. At a certain moment, the server, on the other end, reset the connection. Now we have to throw the connection out from the pool.\nA pick in the documentation, we see some properties:\nProperty Description maxIdleTime The time after which the channel is eligible to be closed when idle (resolution: ms). Default: max idle time is not specified maxLifeTime The total lifetime after which the channel is eligible to be closed (resolution: ms). Default: max lifetime is not specified evictInBackground When this option is enabled, each connection pool regularly checks for connections that are eligible for removal according to eviction criteria like maxIdleTime. By default, this background eviction is disabled ‚úÖ Solution We have to tell the reactor what connections are invalid. We can do that by setting up a ConnectionProvider with some properties. By setting the evictInBackground in combination with maxIdleTime we tell the reactor to clear connections idle for 30 seconds on every 120 seconds. Bellow an example.\nWe can also consider a retry mechanism when the connection is invalid and create a new connection or finally get back to good old RestTemplate, which is blocking and uses one connection at the time\nSimplified for bravery.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Configuration class WebClientConfiguration { private static final Logger log = LoggerFactory.getLogger(WebClientConfiguration.class); @Bean WebClient yourWebClient(Environment config) { ConnectionProvider connectionProvider = ConnectionProvider.builder(\u0026#34;http-connection-pool\u0026#34;) .maxConnections(50) //a number of connections .maxIdleTime(Duration.ofSeconds(30))) //max time to stay in the pool in idle state .maxLifeTime(Duration.ofHours(3))) //max lifetime 3 hours then closed .pendingAcquireTimeout(Duration.ofSeconds(60))) //try to aquire for 60 seconds or else fail .evictInBackground(Duration.ofSeconds(120))) //regularly checks for connections each 120 seconds .metrics(true) //for actuator .build(); HttpClient httpClient = HttpClient.create(connectionProvider) .responseTimeout(Duration.ofSeconds(10))) //close connection, when no data was read within 10 seconds after the socket was initialized. .compress(true) .wiretap(HttpClient.class.getName(), LogLevel.DEBUG, AdvancedByteBufFormat.SIMPLE) //print wiretap logs only when HttpClient logger is on DEBUG .metrics(true, Function.identity()); return WebClient.builder() .clientConnector(new ReactorClientHttpConnector(httpClient)) .baseUrl(\u0026#39;baseUrl\u0026#39;) .build(); } } Optional settings maxLifeTime is just a precaution if there is an exchange of data for 3 hours. pendingAcquireTimeout is how much time to try to acquire a connection - the default is 45 seconds. metrics this is handly for monitoring the throughput and connection pool details. responseTimeout this one is about how much time you have to wait for data after the connection was initialiazed\nThat\u0026rsquo;s pretty much it. You just have to monitor the throughput, see if a tweak of settings is needed. The reactor connection pool is pretty the same as a JDBC connection pool.\nBtw, the same strategy can be applied for the famous prematurely closed exception cases. We could also improve the logging by printing the exception and the connection id on the exception stack trace using ClientRequest.logPrefix().\nüìö Read more The documentation How to Avoid Common Mistakes When Using Reactor Netty\n","permalink":"https://dcebotarenco.github.io/posts/reactor-connection-pool/","summary":"üßë‚Äçüéì tltr Configure evictInBackground, maxIdleTime and maxLifeTime to clear connections from the connection pool or retry the call or use RestTemplate\nüêõ Issue At one of my clients, it was decided no more RestTemplate, all move to üçÉ Spring WebClient. I consider this project very interesting, but it comes with a different mindset which developers do not pay attention to:\nNon-blocking Event driven Connection pool Back to the client. After each deployment, in a matter of days, we were getting - the connection reset by the peer.","title":"üçÉ Spring WebClient \u0026 Connection Pool"},{"content":" All companies have a delivery pipeline of the product. Every company can create its own pipeline and choose any strategy to deliver. In this post I‚Äôm going to talk about NSIS and Python. I had a task to think about the product deployment in the company I‚Äôm working for. Before I got this task, we used to copy paste files via RDP and VPN to the client server and install them manually. Copying the files was implying many risks like:\nüö´ A file is missing!\nüö´ Where is Next, Next, Next, Accept, Install?\nüö´ Developer does not know the procedure of installing the product!\nüö´ What should developer do with the database dump?\nüö´ What environment variables are needed for the product to run?\nFor the client, we can had different solutions that might look like a pipeline:\nWe could build a full continuous delivery to the client server (from Jenkins to Production) We could host client‚Äôs product on our hardware or we could deploy it somewhere in a cloud service, like Azure, AWS\u0026hellip; We could create an .exe file and install it on the client server; Continue copy-pasting the files. In order to ensure a full continuous delivery we need either to host the client‚Äôs product ourselves or to run the client products in cloud, but this had its own disadvantages for this particular client:\nClient had his intranet and different security policies around it He considered expensive hardware or expensive traffic Client had only Windows virtual machines Internally, at that time, we had a project, which I investigated to see how it was built, written in NSIS that was preparing the DEV environment. Therefore, I decided to have a look at NSIS and build an EXE that would install our product on the client server. Having an .exe does not require many costs and it is relatively simple to do next, next, next and install for everyone from service department.\nNSIS is a script-driven Installer authoring tool for Microsoft Windows with minimal overhead backed by Nullsoft, the creators of Winamp. NSIS is released under a combination of free software licenses, primarily the zlib license. Well, after a couple of weeks, I‚Äôve written the installer and it looks like this:\nEach customer project has a configuration file, in which the developer fills what third party components the project needs. Everything was fine, we used the NSIS installer to set up a couple of projects, but then we realized that nobody except me knew how this works and how NSIS looks like. I was always asked to have a look at why something was not installed or crashed during the installation (not the perfect code). Why? Because of NSIS. For other developers this was a quite unknown scripting language. In addition, I can say that this is also a complex language with a complex syntax. We were all Java developers that‚Äôs why everyone avoided it.\nWhy was NSIS a pain in the üçë? Pop, Push, Exch‚Ä¶ The Stack and shared $0-$9 $R0-$R9 registers There is a stack where you have to push and pop values and a couple of registers where you can save some specific values. I was using the registers for time and date because I‚Äôve created a custom logger in NSIS, which was writing logs into a file at runtime. I did not like NSIS because it does not have variables that we are used to in Java.\nComplex looping in stack When it came up to use some loops, I realized that this is very different from a simple PASCAL loop. Even in PASCAL, it is easier to write a loop. Here is an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 function FileScanner.searchFNC pop $FileScanner.callBackAddress pop $FileScanner.getMask pop $FileScanner.directoryToSearch ${locate::Open} \u0026amp;quot;$FileScanner.directoryToSearch\u0026amp;quot; `$FileScanner.getMask` $FileScanner.getHandle StrCmp $FileScanner.getHandle 0 0 loop MessageBox MB_OK \u0026amp;quot;Error\u0026amp;quot; IDOK close loop: ${locate::Find} $FileScanner.getHandle $FileScanner.getPathName $FileScanner.getPath $FileScanner.getName $FileScanner.getSize $FileScanner.getTime $FileScanner.getAttr /*MessageBox MB_OKCANCEL \u0026#39;$$R1 \u0026amp;quot;pathname\u0026amp;quot; =[$R1]$n $$R2 \u0026amp;quot;path\u0026amp;quot; =[$R2]$n $$R3 \u0026amp;quot;name\u0026amp;quot; =[$R3]$n $$R4 \u0026amp;quot;size\u0026amp;quot; =[$R4]$n $$R5 \u0026amp;quot;time\u0026amp;quot; =[$R5]$n $$R6 \u0026amp;quot;attributes\u0026amp;quot; =[$R6]$n $n\u0026#39;*/ ${Logger.log} \u0026amp;quot;Found : $FileScanner.getPathName\u0026amp;quot; ${If} $FileScanner.getName == \u0026amp;quot;\u0026amp;quot; goto close ${Else} call $FileScanner.callBackAddress ${EndIf} goto loop close: ${locate::Close} $FileScanner.getHandle ${locate::Unload} functionend No debug I hadn‚Äôt managed to find a good way of debugging a NSIS script at runtime. I had found a plugin called Dumpstate which was stopping the installer at the breakpoint and showing me the state of the registries and the stack. However, it was very difficult to understand why it was failing at certain points.\nNo OOP In NSIS I could not use the OOP principles. I was not able to create objects. I had to simulate it with macros and separate files that had getters.\n1 2 3 4 5 6 7 8 9 10 11 12 function ActiveMQ.setup ${Logger.log} \u0026amp;quot;Installing ActiveMQ\u0026amp;quot; DetailPrint \u0026amp;quot;Installing ActiveMQ\u0026amp;quot; ${Logger.log} \u0026amp;quot;Copy ActiveMQ file to directory $DirectoryPage.getUserActiveMQDirectory\u0026amp;quot; CopyFiles /SILENT \u0026#39;$EXEDIR${ComponentLocation.activemq}*.*\u0026#39; \u0026#39;$DirectoryPage.getUserActiveMQDirectory\u0026#39; nsExec::ExecToLog \u0026amp;quot;$DirectoryPage.getUserActiveMQDirectorybinwin$System.getSystemBitTypeInstallService.bat\u0026amp;quot; pop $1 ${System.checkScriptStatus} \u0026amp;quot;$DirectoryPage.getUserActiveMQDirectorybinwin$System.getSystemBitTypeInstallService.bat\u0026amp;quot; $1 pop $0 ${Logger.log} \u0026amp;quot;ActiveMQ was installed successfuly\u0026amp;quot; DetailPrint \u0026amp;quot;ActiveMQ was installed successfuly\u0026amp;quot; functionEnd Plugins never work During the development, I was always searching for plugins that would save time and shorten the code. Sadly it wasn‚Äôt always the case. Every time I tried a new plugin it was failing. Besides this, the documentation for some of them was missing. Each project has its own .nsis files inside the Java module that has to be compiled via a Maven goal to get the .exe file. So, the pipeline was bounded to the NSIS compiler when we were preparing the install package. This was time consuming and the DEV machine had to have a compiler installed.\nPython NSIS was a mistake, and I admit it. Half a year later, I decided that because I was the one who has created the installer with NSIS and it was so hard to maintain, I should be the one who will change it! It is time to use another language and another approach. First, I thought to remove the pre-processing steps like compiling .nsis files to have the install pack faster. I‚Äôve decided to make the installer as a separate project.\nI‚Äôve decided to build it in Python, I want to learn it. Python had everything I needed:\nGUI libs OOP Easy syntax Widely supported Extensible. The main idea remained the same. The client project has the same file with the dependencies it requires. The only difference is that for now Python has to be installed on the production server to run the installer. How to do that? Simple‚Ä¶ self-extracting archive solution provided by 7ZIP. You can configure a .bat file to run after the extraction has been finished. The procedure of creating the install pack also remained the same. Nobody even noticed anything.\nHere is how the new installer looks like: Conclusion Think twice when it comes to choose a development language. I really enjoyed working in Python and building the installer from scratch rather than in NSIS. Having the installer as a modern separate project makes it easy to maintain and improve not only by me, but by other developers as well, who know python.\n","permalink":"https://dcebotarenco.github.io/posts/nsis-vs-python/","summary":"All companies have a delivery pipeline of the product. Every company can create its own pipeline and choose any strategy to deliver. In this post I‚Äôm going to talk about NSIS and Python. I had a task to think about the product deployment in the company I‚Äôm working for. Before I got this task, we used to copy paste files via RDP and VPN to the client server and install them manually.","title":"üß∞ Nsis vs Python"},{"content":"We all tend to do our work faster because time costs money. I realized that we lose a lot of time waiting for IDE to start. I use Netbeans IDE and all our work is related to it. For Java modules, we use Maven and, of course, we have some custom goals. Some goals are pure Java, though we have some goals that use SVN for committing, logging, reverting and so on. And some of them are related to our RDBMS as we load a DB.\nBut the main problem is that we can do all of this only when Netbeans IDE finishes its class path scanning and other indexing background processes. This, depending on your hardware, could sometimes take up to 3-5 minutes. I didn‚Äôt like it, so I decided to make this process a bit faster.\nThe following main features we use, which I could improve:\nLoad database SonarQube scanning Clean and build Pull the source code aka Git but this is for SVN Push the source code aka Git but this is for SVN SVN reset SVN log Java Deploy All these features are related to Maven, therefore, we can write Maven commands. Here is where the BASH comes in! We all know that bash is a very popular and flexible scripting language used by many people around the world. I decided to install and use it instead of the default CMD from Windows. However, it is really hard to remember and write a command like:\n1 mvn clean install myPlugin:loaddb myPlugin:execmodule myPlugin:migratedb myPlugin:activemq -Pdevelopment -Dactivemq.purgeAllQueues=true I could have used some plugins from GitHub that provide some features, like fast running a command. But, since I am not a terminal at this moment person, I decided to write my own bash scripts so I could get a taste of it. This would introduce me to the bash world and give me a brief view of its commands. As starting point I installed Git that provides, by default, the Bash terminal and its environment.\nBash aliases Bash terminal can be configured to load a file at startup. This file is called .bash_profile. You can create it in the %USER_PROFILE% directory. In this file, you can write your own bash scripts and they will be available in the terminal at runtime. I added some aliases that are bound to some functions I could run. Example:\n1 2 3 alias r=\u0026#39;r_registerCurrentDirectoryWithAnAllias\u0026#39; alias j=\u0026#39;j_jumpToADirectory\u0026#39; alias m=\u0026#34;maven_command\u0026#34; Each alias value is a Bash function that is responsible for doing something. Taking in consideration that all commands are Maven based, I defined three aliases I would need:\nM ‚Äì Run a maven command based on a pom.xml file R ‚Äì Stands for register. I use it to add a directory with a directory alias, list them or remove one J ‚Äì Jump to a directory using a directory alias Core and customer structure Before we go through the implementation, I have to explain why I defined these specific aliases. Internally, we build a product for clients that hold warehouses. More or less, they all have mostly the same functionality, so we have divided our product in 2 parts:\nThe Core (common functionality) The Client (customer specific functionality) Because of this, we have different versions for these components. The Core is based on branches and tags like 1.0-SNAPSHOT or 1.0.1.\nThe Client component is based on environments:\nDevelopment-SNAPSHOT, where it may have a dependency on 1.0-SNAPSHOT Acceptance-SNAPSHOT Production-SNAPSHOT This structure creates some patterns for accessing specific project folder, like:\n1 2 3 4 5 D\\:ProjectA\\branches\\1.0-SNAPSHOT D\\:ProjectB\\tags\\1.0.2 D\\:ProjectC\\environment\\development D\\:ProjectC\\environment\\acceptance D\\:ProjectC\\environment\\production Commands R ‚Äì Register Now let‚Äôs see what was optimized by using Bash. Using the above mentioned aliases, I can run the commands. As I mentioned r stands for register, which holds different aliases for different paths. Using r I can run:\na ‚Äì add alias; ls ‚Äì list aliases; r ‚Äì remove an alias. Main function:\n1 2 3 4 5 6 function r_registerCurrentDirectoryWithAnAllias() { command=$1 alias=$2 r_processCommand $command $alias } Delegated function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 function r_processCommand() { command=$1 alias=$2 validateParameter $command local isCommandValid=$? if [ $isCommandValid == 0 ] then case $command in \u0026#34;a\u0026#34; ) validateParameter $alias local isAlliasValid=$? if [ $isAlliasValid == 0 ] then add $alias else echo \u0026#34;Missing alias\u0026#34; fi ;; \u0026#34;ls\u0026#34; ) initFile out_pathDir pathDir=$out_pathDir fileLines $pathDir 0 ;; \u0026#34;r\u0026#34; ) validateParameter $alias local isAlliasValid=$? if [ $isAlliasValid == 0 ] then remove $alias else echo \u0026#34;Missing alias\u0026#34; fi ;; esac else echo \u0026#34;Missing command\u0026#34; fi } Now I go to the pom.xml file of the project and run: r a test ‚Äì which means - register add for the current path the alias test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 function add() { initFile out_pathDir pathDir=$out_pathDir alias=$1 currentDir=$(pwd) fileLines $pathDir 1 file_lines_array_size=${#fileLines_lines[@]} if [ $file_lines_array_size == 0 ] then echo \u0026#34;$alias=$currentDir\u0026#34; \u0026amp;gt;\u0026amp;gt; $pathDir else array_of_lines=${fileLines_lines[@]} for line in $array_of_lines do lineKeyValue $line out_key out_value if [ $out_value == $currentDir ] then echo \u0026#34;Such directory already exists on $line\u0026#34; break else echo \u0026#34;$alias=$currentDir\u0026#34; \u0026amp;gt;\u0026amp;gt; $pathDir break fi done fi } This command creates in background a file .pathDir which will be used as a storage for all the aliases. Now I have an alias inside it.\nIn order to see what other aliases I have, I can write: r ls ‚Äì this command will display all the aliases I have in the .pathDir\nAnd of course, I can remove an alias by writing: r r test ‚Äì which means: register remove the alias test linked to any path. Now I can use test for any other path from the system.\n1 2 3 4 5 6 7 8 9 function remove() { initFile out_pathDir pathDir=$out_pathDir alias=$1 currentDir=$(pwd) sed \u0026#34;/$alias=/d\u0026#34; $pathDir \u0026gt;\u0026gt; \u0026#34;$pathDir tmp\u0026#34; \u0026amp;\u0026amp; mv \u0026#34;$pathDir tmp\u0026#34; $pathDir } J ‚Äì Jump After we added an alias for a path, we can now directly jump to that directory by typing: j iut ‚Äì it‚Äôs a simple cd to the referenced path.\nMain function:\n1 2 3 4 5 6 function j_jumpToADirectory { alias=$1 branch=$2 j_jump $alias $branch } However, this does not bring you a lot of magic since we might have tags, branches and environments for a project. Well, jump supports additional parameters to the command, like: j iut 1.0.2 ‚Äì this way we can jump specifically inside the needed folder. But in this case iut alias should be linked to ‚ÄòD:\\ProjectA\\‚Äô , because this will be the root folder of the project. We can also jump to a tag or an environment if it exists, by writing the following command: j iut 1.0.2 or j iut d where d stands for development environment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 function j_jump { local alias=$1 local branch=$2 validateParameter $alias local isAliasValid=$? if [ $isAliasValid == 0 ] then initFile out_pathDir pathDir=$out_pathDir fileLines $pathDir 1 file_lines_array_size=${#fileLines_lines[@]} if [ $file_lines_array_size != 0 ] then array_of_lines=${fileLines_lines[@]} local isFound=1 for line in $array_of_lines do lineKeyValue $line out_key out_value if [ $out_key == $alias ] then isFound=0 local path=$out_value validateParameter $branch local isBranchValid=$? if [ $isBranchValid != 0 ] then cd $path break else cd $path j_process_branch $branch out_path local fullpath=\u0026#34;$path/$out_path\u0026#34; cd $fullpath break fi fi done if [ $isFound == 1 ] then echo \u0026#34;No such alias found\u0026#34; fi else echo \u0026#34;No aliases found\u0026#34; fi else echo \u0026#34;Missing parameter\u0026#34; fi } function j_process_branch { local branch=$1 local pth=$2 isMajorMinorFunc $branch local isMajorMinor=$? isTagFunc $branch local isTag=$? isPrivateNumberFunc $branch local isPrivateBranch=$? if [ $isMajorMinor == 0 ] then eval $pth=\u0026#34;branches/$branch-SNAPSHOT\u0026#34; elif [ $isTag == 0 ] then eval $pth=\u0026#34;tags/$branch\u0026#34; elif [ $isPrivateBranch == 0 ] then eval $pth=\u0026#34;branches//private-$branch\u0026#34; else case $branch in \u0026#34;t\u0026#34; ) eval $pth=\u0026#34;branches/TRUNK-SNAPSHOT\u0026#34; ;; \u0026#34;d\u0026#34; ) eval $pth=\u0026#34;environment/development\u0026#34; ;; \u0026#34;a\u0026#34; ) eval $pth=\u0026#34;environment/acceptance\u0026#34; ;; \u0026#34;p\u0026#34; ) eval $pth=\u0026#34;environment/production\u0026#34; ;; esac fi } M ‚Äì Maven Now, when we are able to jump faster from one project to another, we can run maven goals over the pom.xml files, like this:\nm ldb ‚Äì loads database m pull ‚Äì SVN update goal m push ‚Äì SVN commit goal m reset ‚Äì SVN revert goal m log ‚Äì SVN log goal m deploy ‚Äì deploy m sonar ‚Äì sonar goal 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 function maven_command { local command=$1 validateParameter $command local isCommandValid=$? if [ $isCommandValid == 0 ] then case $command in \u0026#34;sonar\u0026#34; ) mvn clean install sonar:sonar -Dsonar.host.url=\u0026#34;https://sonarqube-host:9000\u0026#34; ;; \u0026#34;ldb\u0026#34; ) local setup=$(find -regex \u0026#34;.*setup$\u0026#34; | head -1) cd \u0026#34;$setup\u0026#34; mvn clean install myPlugin:loaddb myPlugin:execmodule myPlugin:migratedb myPlugin:activemq -Pdevelopment -Dactivemq.purgeAllQueues=true ;; \u0026#34;ci\u0026#34; ) mvn clean install ;; \u0026#34;pull\u0026#34; ) mvn myPlugin:ts-update -N ;; \u0026#34;push\u0026#34; ) mvn myPlugin:ts-commit -N ;; \u0026#34;reset\u0026#34; ) mvn myPlugin:ts-revert -N ;; \u0026#34;log\u0026#34; ) mvn myPlugin:ts-log -N ;; \u0026#34;deploy\u0026#34; ) mvn clean source:jar javadoc:jar deploy -Pjavadocprof ;; *) mvn $@ ;; esac else echo \u0026#34;Missing command\u0026#34; fi } Conclusion Having all of this configured, we can run the load the database or any command over a pom.xml file without even having to start the Netbeans IDE. The .bash_profile can be customized per developer, so everyone can add commands of their own. The most important thing is that using Bash we can gain a lot of power due to its internal commands that Windows does not have. We save a lot of time. And while Netbeans starts, you could already have 3-4 terminals working:\nOne loading the DB Another loading logs Third updating another project ","permalink":"https://dcebotarenco.github.io/posts/power-of-bash/","summary":"We all tend to do our work faster because time costs money. I realized that we lose a lot of time waiting for IDE to start. I use Netbeans IDE and all our work is related to it. For Java modules, we use Maven and, of course, we have some custom goals. Some goals are pure Java, though we have some goals that use SVN for committing, logging, reverting and so on.","title":"üí™ Power of Bash"},{"content":"Intro Once I read a post about a guy who was searching for an apartment in San Francisco. He was annoyed about the fact that searching for ‚ÄúHow to find an apartment in San Francisco‚Äù on Google yields dozens of pages of advice as a good indicator of apartment hunting and this is a painful process.\nSo he decided to implement a bot in Slack that would help him to search an apartment based on scraping data from websites and notifying him about the cheapest and closest apartment around him. His idea inspired me to create a bot, although I didn‚Äôt know at that time what are the functionalities I am going to build.\nThe idea We started brainstorming, suddenly in the Skype chat I saw ‚ÄúDon‚Äôt forget to order food‚Äù. Let me explain, internally we ordered food from a local catering company and our office manager each morning was pasting this message. We have a google sheet where we store our choices. Later, based on choices, the office manager places an order. That is it! We need a bot that would order food for us. A lad charea joined me so we could do it twice as fast (we thought).\nChoosing the right tools Together we started to investigate on what platforms a bot can be built and we found a quite new and fresh platform, that is developed by Microsoft, called Microsoft Bot framework. We have found it very interesting and powerful. The only thing we had to do is to register on their platform. They provide an API in NodeJS and C#. We haven‚Äôt spent a lot of time choosing. Since we do Java and C# is almost the same, we decided to go for NodeJS because it was something new for us. If we had chosen C#, then the development of the bot would not have been so funny and challenging.\nMicrosoft Bot framework It enables organizations to build intelligent agents, known as Bots. The Framework provides developers with a developer portal and SDK to build Bots, a Bot Connector service to connect to social channels such as Twitter and Slack, and a Bot Directory (Marketplace) to discover and use existing bots It supports Node JS and C#. It provides a powerful session management and some interactive cards like Hero, Thumbnail, Receipt and Sign-In. NodeJS \u0026amp; ES6 We will not speak a lot about NodeJS, but there is something to mention:\nAn important thing to realize is that Node is not a web server. By itself it doesn‚Äôt do anything. It doesn‚Äôt work like Apache. There is no config file where you point it to your HTML files. If you want it to be a HTTP server, you have to write an HTTP server (with the help of its built-in libraries).\\ Node.js is just another way to execute code on your computer. It is simply a JavaScript run time; ECMAScript 6 is a new version of ECMAScript providing a new set of features and fixes to JavaScript. IDE This was another tool we had to decide on. There aren‚Äôt a lot of free good IDEs that would support NodeJS. Most popular found:\nWebstorm (30 days) Netbeans 8.2 (free) Eclipse (using some plugins) Sublime Atom We went for Webstorm and Netbeans. Btw Webstorm is the best. Bot Framework simulator Another tool we have had is the simulator provided by the platform. This tool is simulating a REST server with which our bot communicates using the predefined API. It is like a local Skype server. This gives you the possibility to chat with your bot locally without deploying it. Using the Emulator, you can:\nSend requests and receive responses to/from your bot endpoint on localhost; Inspect the JSON response; Emulate a specific user and/or conversation. Google API In order to be able to access Google Spreadsheets we needed to find a way of interacting with our application, and luckily for us, Google officially supported a Node.js client library for accessing Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT is included. At this point we had to choose which type of authentication fits best for us. Since we needed a type of authentication in which the end-user would not be involved in the process of generating tokens to establish the identity within Google API, we had chosen JWT(JSON Web Token).\nTokens are generated for a service account, which is created from the Google API console. Service accounts must also be granted access to resources, using traditional assignation of permissions using the unique service account email address. The application requests a token that can be used for authentication in exchange with a valid JWT. The resulting token can then be used for multiple API calls, until it expires and a new token must be obtained by submitting another JWT.\nThe Bot Mainly our bot gathers user choices regarding lunch options and places orders in the Google Sheet. To use the bot it is necessary to install it via a Skype, join URL and register yourself. The registration itself is implemented in a very simple way. We have two locked columns in the Google Sheet. First column is the ID and the second one is the Skype name. For each Skype name we have a specific ID. This ID is provided by the Skype service. After the user completes the registration, he/she is able to use the bot. The bot provides some dialogs that help the user choosing his/her meal:\nInteractive Dialogs: Greeting Dialog Help Place Order Cancel Orders Status Non interactive Dialogs: Jokes Let‚Äôs have a closer look at them. Greeting Dialog This dialog provides a greeting message together with the user ID that needs to be sent to the administrator. The trigger keyword is ‚Äòhi‚Äô. Help Dialog Dialog that shows to the user all commands that can be entered. The trigger keyword is ‚Äòhelp‚Äô. Place Order Dialogs If the user is hungry then he/she has to place the order by writing ‚Äòfood‚Äô The Bot will show the menu for the current day. If the user wants to order food for another day then the command should be like : ‚Äòfood (today|mo|tu|we|th|fr)‚Äô It is possible to order more than 1 meal per day. Number of orders per one day is equal with the number of rows related to the user in the Google Sheet If the user has 1 meal and it‚Äôs completed it will be overridden If the user has 2 or more meals and these are completed, the bot will ask him to cancel one of them This Dialog is triggered by a cron expression that is running in background Cancel Order Dialog In case the user decided to cancel an order, he/she has to write: ‚Äòfood cancel (today|mo|tu|we|th|fr)‚Äô by specifying the day for which the he/she wants to cancel the order If the user has more than 1 meals on that day, the bot will ask him to cancel 1 meal Status Dialog In order to see what is the current choice, the user can write ‚Äòfood status‚Äô The Bot will show the user‚Äôs today choices If the user wants to see the food status for another day then the ‚Äòfood status (today|mo|tu|we|th|fr)‚Äô command can be used. Heroku integration and hot deploy You may ask yourself where do we run it. We have chosen the PaaS platform HEROKU that is very easy to use:\nHeroku is a cloud platform that lets companies build, deliver, monitor and scale apps ‚Äî we‚Äôre the fastest way to go from idea to URL, bypassing all those infrastructure headaches; Heroku is a cloud platform as a service(PaaS). That means you do not have to worry about infrastructure, you just focus on your application.\nHere are some features of Heroku Instant Deployment with Git push ‚Äì the build of your application is performed by Heroku using your build scripts Plenty of Add-on resources (applications, databases etc.) Processes scaling ‚Äì independent scaling for each component of your app without affecting functionality and performance Isolation ‚Äì each process (aka dyno) is completely isolated from each other; Full Logging and Visibility ‚Äì easy access to all logging output from every component of your app and each process (dyno); Heroku provides very well written tutorial which allows you to start in minutes. Also they provide first 750 computation hours free of charge which means you can have one processes (aka Dyno) at no cost. Also performance is very good e.g.a simple web application written in Node.js can handle around 60 ‚Äì 70 requests per second. Publishing the bot Publishing the bot basically means to add it to the Bot Directory which is a public directory of all bots that were registered and published within the Microsoft Bot Framework. Users can select your bot in the directory and add it to one or more of the configured channels that they use. After you publish your bot, Microsoft will review your bot submission to make sure it meets a certain minimum requirements before it is publicly available on the Bot Directory. Although, the bot can be used without being published, there is a limit of 100 users.\n","permalink":"https://dcebotarenco.github.io/posts/skype-bot/","summary":"Intro Once I read a post about a guy who was searching for an apartment in San Francisco. He was annoyed about the fact that searching for ‚ÄúHow to find an apartment in San Francisco‚Äù on Google yields dozens of pages of advice as a good indicator of apartment hunting and this is a painful process.\nSo he decided to implement a bot in Slack that would help him to search an apartment based on scraping data from websites and notifying him about the cheapest and closest apartment around him.","title":"ü§ñ Skype bot"},{"content":"Intro üíÅ I\u0026rsquo;m Dan. Code for food 10+ years. Mainly focused on Java, JVM, frameworks and languages around it. Practicing Extreme Programming and clean code. I like table tennis, volleyball and books\n","permalink":"https://dcebotarenco.github.io/about/","summary":"Intro üíÅ I\u0026rsquo;m Dan. Code for food 10+ years. Mainly focused on Java, JVM, frameworks and languages around it. Practicing Extreme Programming and clean code. I like table tennis, volleyball and books","title":""}]